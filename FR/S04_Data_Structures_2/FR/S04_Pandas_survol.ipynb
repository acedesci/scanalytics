{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S04 - Pandas - Survol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de ce fichier est de vous fournir un récapitulatif des différents enseignements que vous avez vus pendant le [cours Kaggle sur pandas](https://www.kaggle.com/learn/pandas). `pandas` est une librairie (tierce) populaire pour analyser et manipuler les données. Construite sur le langage de programmation Python, c'est un outil rapide, puissant, flexible et facile à utiliser.\n",
    "\n",
    "**Notez que la plupart des opérations dans `pandas` vous renvoient un nouvel objet. Ces opérations ne sont généralement pas effectuées en-place. Ainsi, si vous souhaitez conserver vos résultats, vous devez les assigner à une variable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création, lecture et écriture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour utiliser la librairie `pandas`, vous devez d'abord l'importer; vous ne pouvez et ne devez l'importer qu'une seule fois par `Jupyter Notebook`. Vous pouvez le faire de différentes façons (tout comme pour d'autres librairies). La première approche est la suivante:\n",
    "``` python\n",
    "import pandas\n",
    "```\n",
    "Ce faisant, vous devez ensuite écrire `pandas` devant tous les éléments `pandas` auxquels vous souhaitez accéder. Par exemple, pour créer un `DataFrame`, vous devez effectuer l'opération suivante:\n",
    "``` pyton\n",
    "pandas.DataFrame({'Yes':[50, 21], 'No':[131, 2]})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "La deuxième approche consiste à importer la librairie sous un alias (c.-à-d., un nom différent). Une convention lors de l'importation de `pandas` est de l'importer sous le nom `pd`. Vous devez tout de même ensuite écrire `pd` devant tous les éléments `pandas` auxquels vous souhaitez accéder, mais cela permet de réduire le nombre de lettres à écrire. Par exemple, le code précédent se réduirait à:\n",
    "``` python\n",
    "import pandas as pd\n",
    "pd.DataFrame({'Yes':[50, 21], 'No':[131, 2]})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "La troisième approche consiste à importer uniquement les éléments de la librairie dont vous avez besoin. Par exemple, si vous avez seulement besoin de l'objet `DataFrame`, vous pouvez procéder de la façon suivante:\n",
    "``` python\n",
    "from pandas import DataFrame\n",
    "DataFrame({'Yes':[50, 21], 'No':[131, 2]})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Importons `pandas` en utilisant la deuxième approche:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataFrame` et `Series`\n",
    "Les deux principaux objets d'intérêt dans la librairie `pandas` sont les objets` DataFrame` et `Series`.\n",
    "\n",
    "---\n",
    "Il est possible de créer un `DataFrame` en fournissant un dictionnaire où les clés sont les étiquettes des colonnes et les valeurs sont des listes contenant les différents éléments de la colonne correspondante. Notez que toutes les listes doivent être de la même longueur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Yes':[50, 21], 'No':[131, 2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible de créer une `Series` en fournissant une liste. Une `Series` peut être considérée comme une colonne d'un `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([4, 5, 2, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer des données\n",
    "La plupart du temps, les données ne sont pas entrées manuellement dans le constructeur, mais elles sont plutôt importées à partir d'un fichier externe. Cela peut être fait en utilisant les commandes `read_csv()` pour les fichiers CSV (*Comma-Separated Values* ou valeurs séparées par des virgules) ou `read_excel()` pour les fichiers Excel. D'autres options sont également disponibles dans la documentation `pandas`.\n",
    "\n",
    "Importons maintenant un fichier CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import la colonne WEEK_END_DATE en tant que date\n",
    "url = 'https://raw.githubusercontent.com/acedesci/scanalytics/master/data/salesCerealsOriginal.csv'\n",
    "df = pd.read_csv(url, parse_dates=['WEEK_END_DATE']) \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici une description des variables du précédent `DataFrame`. Notez que ce fichier sera également réutilisé aux séances suivantes.\n",
    "\n",
    "| VARIABLE | DESCRIPTION | \n",
    "|:----|:----|\n",
    "|WEEK_END_DATE|date de fin de semaine|\n",
    "|STORE_NUM|numéro de magasin|\n",
    "|UPC|identifiant spécifique au produit (*Universal Product Code*)|\n",
    "|UNITS|nombre d'unités vendues|\n",
    "|VISITS|nombre d'achats uniques (paniers) comprenant le produit|\n",
    "|HHS|nombre de ménages acheteurs|\n",
    "|SPEND|total dépensé (c.-à-d., $ ventes)|\n",
    "|PRICE|montant réel facturé pour le produit en rayon|\n",
    "|BASE_PRICE|prix de base de l'article|\n",
    "|FEATURE|indication de si le produit était dans la circulaire du magasin|\n",
    "|DISPLAY|indication de si le produit faisait partie de l'affichage promotionnel en magasin|\n",
    "|TPR_ONLY|indication de si le produit était en réduction de prix temporaire (c.-à-d., étiquette d'étagère uniquement; pas sur l'affichage ou dans la circulaire)|\n",
    "|DESCRIPTION|description du produit|\n",
    "|CATEGORY|catégorie du produit|\n",
    "|SUB_CATEGORY|sous-catégorie du produit|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporter des données\n",
    "L'exportation de données peut être effectuée avec des méthodes analogues telles que `to_csv()` et `to_excel()`.\n",
    "\n",
    "Par exemple, pour exporter le `DataFrame` en fichier *CSV*, on utilise le code suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"temp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par la suite, si on est sur Google Colab, on doit exécuter le code suivant du module `files` afin de télécharger le fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "files.download(\"temp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexation, sélection et affectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez accéder à toutes les colonnes d'un `DataFrame` en utilisant la notation par point ou en utilisant les crochets `[]`. Ensuite, vous pouvez accéder à une ligne spécifique dans cette colonne en utilisant à nouveau les crochets `[]` avec le numéro de la ligne. Notez que cette seconde indexation peut parfois conduire à des résultats étranges selon la façon dont les index sont définis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.UPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['UPC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.UPC[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['UPC'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection basée sur un index\n",
    "Une autre façon d'accéder aux éléments d'un `DataFrame` est d'utiliser la **sélection basée sur un index**, c'est-à-dire la méthode `iloc[]`. La sélection se fait par le numéro d'index de la ligne et de la colonne. Par exemple, pour accéder à la première ligne de la colonne numéro 3 (c'est-à-dire, la colonne 'UPC'), nous écrivons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection basée sur les étiquettes\n",
    "Une troisième option permettant d'accéder aux éléments d'un `DataFrame` est d'utiliser la **sélection basée sur les étiquettes**, c'est-à-dire la méthode `loc[]`. La sélection se fait par les étiquettes de ligne et de colonne. En revenant à notre notre dernier exemple, nous écrivons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, 'UPC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est également possible d'utiliser la méthode `loc[]` avec des masques booléens. Par exemple, pour afficher toutes les lignes liées à l'UPC 1111085319, nous pouvons écrire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.UPC == 1111085319]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces expressions booléennes peuvent être combinées avec l'esperluette (`&`) permettant d'appliquer l'opérateur `and` sur chacun des éléments, et le tube (`|`) permettant la même chose avec l'opérateur `or`. La méthode `isin()` peut également être utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df.UPC == 1111085319) & (df.FEATURE == True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin, pour trouver des valeurs nulles et non nulles, vous pouvez utiliser les méthodes `isnull()` et `notnull()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions récapitulatives et *mappings*\n",
    "## Fonctions récapitulatives\n",
    "Certaines fonctions récapitulatives intéressantes sont les méthodes `describe()`, `unique()` et `value_counts()`:\n",
    "\n",
    "- **Méthode `describe()`**\n",
    "Cette méthode permet de visualiser certains détails statistiques de base comme la moyenne, la variance, les percentiles, etc.\n",
    "\n",
    "- **Méthode `unique()`**\n",
    "Cette méthode est utilisée pour obtenir des valeurs uniques d'une `Series`.\n",
    "\n",
    "- **Méthode `value_counts()`**\n",
    "Cette méthode est utilisée pour obtenir une série de valeurs uniques et le compte de chaque valeur.\n",
    "\n",
    "En exécutant ces méthodes ci-dessous, nous constatons qu'il n'y a qu'un seul magasin et seulement 7 UPC différents dans cet ensemble de données. Nous constatons également que les données ont été compilées sur 156 semaines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.STORE_NUM.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.UPC.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.WEEK_END_DATE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Mappings*\n",
    "### `map`\n",
    "La méthode `map()` est utile pour appliquer une fonction à chaque élément d'un sous-ensemble d'un `DataFrame`. Elle est souvent utilisée avec une expression `lambda`. Avec le mot-clé lambda, il est possible de créer de petites fonctions anonymes; c.-à-d., de définir une fonction qui ne sera pas réutilisée ailleurs dans le code. Une expression `lambda` définit d'abord la ou les entrées sur le côté gauche des deux points (`:`). Ensuite, sur le côté droit des deux points (`:`), il fournit ce qui doit être retourné par cette fonction. *Si votre fonction nécessite plusieurs lignes de code, il peut être préférable de définir une fonction en utilisant une syntaxe standard `def ...` au lieu d'une expression `lambda`.*\n",
    "\n",
    "À titre d'exemple, la fonction suivante:\n",
    "``` python\n",
    "def times_two(x):\n",
    "    return x * 2\n",
    "```\n",
    "est équivalente à l'expression `lambda` suivante:\n",
    "``` python\n",
    "lambda x: x * 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def times_two(x):\n",
    "    return x * 2\n",
    "\n",
    "df.PRICE.map(times_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.PRICE.map(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisons la colonne SPEND\n",
    "# Notez que le résultat n'est pas enregistré car il n'est pas affecté à une variable\n",
    "df.SPEND.map(\n",
    "    lambda x: (x - df.SPEND.mean()) / df.SPEND.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le code précédent équivaut au suivant\n",
    "# où nous n'utilisons pas la méthode map()\n",
    "# Ce code suivant est également généralement plus rapide\n",
    "(df.SPEND - df.SPEND.mean()) / df.SPEND.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `apply`\n",
    "La méthode `apply()` permet de parcourir chaque ligne ou chaque colonne (au lieu de chaque élément comme pour la méthode `map()`). Elle peut également être utilisée avec une expression `lambda`.\n",
    "\n",
    "À titre d'exemple, calculons le rabais en pourcentage ($\\frac{\\mathit{BASE\\_PRICE} - \\mathit{PRICE}}{\\mathit{BASE\\_PRICE}} $) lorsque l'article est dans la circulaire (FEATURE est égal à 1). On ajoute ensuite ce calcul à la colonne REBATE_PERC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebate_perc(row):\n",
    "    if row.FEATURE == 1:\n",
    "        rebate = (row.BASE_PRICE - row.PRICE) / row.BASE_PRICE\n",
    "        return rebate\n",
    "    elif row.FEATURE == 0:\n",
    "        return 0\n",
    "\n",
    "# axis='columns' permet de parcourir chaque ligne\n",
    "# axis='index' permet de parcourir chaque colonne\n",
    "df['REBATE_PERC'] = df.apply(rebate_perc, axis='columns')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regroupement et tri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regroupement\n",
    "La méthode `groupby()` est similaire aux tableaux croisés dynamiques dans Excel. Elle permet de regrouper les éléments par une ou plusieurs dimensions (les zones *Lignes* et *Colonnes* de la fenêtre des paramètres du tableau croisé dynamique). Après avoir regroupé les éléments, vous pouvez ensuite calculer certaines fonctions d'aggrégation sur ces groupes (la zone *Valeurs* de la fenêtre des paramètres du tableau croisé dynamique). Ces fonctions d'aggrégation peuvent être les vôtres; définies par une expression `lambda` ou une définition de fonction standard (en utilisant `def ...`).\n",
    "\n",
    "Comme premier exemple, calculons le prix de vente moyen de chaque UPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('UPC').PRICE.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme deuxième exemple, calculons le nombre de fois où chaque UPC est apparu dans la circulaire. N'oubliez pas que cette variable est une variable binaire/booléenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('UPC').FEATURE.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme troisième et dernier exemple, identifions les prix minimum et maximum pour chaque UPC et chaque valeur possible de la variable FEATURE (lorsque l'article est dans la circulaire, FEATURE est égal à 1). Notez que l'index renvoyé est maintenant un multi-index. Il est possible de déplacer ce multi-index sous forme de colonnes en appliquant la méthode `reset_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['UPC', 'FEATURE']).PRICE.agg([min, max])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tri\n",
    "Il est également possible de trier un `DataFrame` ou une `Series` par une ou plusieurs variables en utilisant la méthode `sort_values()`. Si vous souhaitez plutôt trier sur l'index, vous devez utiliser la méthode `sort_index()`.\n",
    "\n",
    "Par exemple, ci-dessous, nous trions d'abord par PRICE et, s'il y a des égalités, nous trions ensuite par BASE_PRICE. Ce tri se fait par ordre décroissant (`ascending=False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['PRICE', 'BASE_PRICE'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types de données et valeurs manquantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types de données\n",
    "`pandas` attribue différents types aux différentes colonnes. Certains types courants sont:\n",
    "- `int` noté par `int64`\n",
    "- `float` noté par `float64`\n",
    "- `str` noté par `object`\n",
    "- dates noté par `datetime64[ns]`\n",
    "\n",
    "`pandas` peut désigner vos dates comme `object` s'il ne comprend pas que ce sont des dates. Il est important que vos dates soient du type `datetime64[ns]` si vous souhaitez utiliser certaines des fonctions intéressantes disponibles dans `pandas` pour manipuler les dates.\n",
    "\n",
    "Il est possible de vérifier le type d'une colonne en utilisant la propriété `dtype`. Il est également possible de vérifier le type de toutes les colonnes en utilisant la propriété `dtypes` comme ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous souhaitez convertir le type d'une colonne, vous pouvez utiliser la méthode `astype()`.\n",
    "\n",
    "Par exemple, si nous voulons convertir le type de STORE_NUM de `int64` en `float64`, nous procédons comme suit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.STORE_NUM.astype('float64')  # ou utiliser juste float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Données manquantes\n",
    "Il est possible de vérifier s'il y a des valeurs nulles dans une colonne d'un `DataFrame` (ou dans un `DataFrame` complet) en utilisant la méthode `isnull()` (ou son compagnon `notnull()`). Ceux-ci renvoient une valeur booléene indiquant si des valeurs nulles (désignées par `NaN`) sont présentes. Notez que les valeurs `NaN` sont toujours de type `float64`.\n",
    "\n",
    "Vérifions ci-dessous si nous avons des valeurs manquantes dans la colonne SPEND."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SPEND.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.SPEND.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifions maintenant s'il y a des valeurs manquantes dans des colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nous avions trouvé des valeurs manquantes, nous aurions pu les remplacer en utilisant la méthode `fillna()`. Il est également possible de remplacer d'autres valeurs en utilisant la méthode `replace()`.\n",
    "\n",
    "Par exemple, dans la colonne STORE_NUM remplaçons la valeur 367 par du texte (c.-à-d., `'Kwik-E-Mart'`) et affichons ce texte dans une nouvelle colonne. Notez ici qu'il n'y a qu'un seul numéro de magasin dans les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STORE_TXT'] = df.STORE_NUM.replace(367, 'Kwik-E-Mart')  # la nouvelle colonne se retrouve à droite des colonnes existantes\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renommage et combinaison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renommage\n",
    "Il est également possible de renommer des index ou des colonnes en utilisant la méthode `rename()`. Une manière élégante d'utiliser cette méthode est de fournir un dictionnaire où les clés sont les index ou les étiquettes courants des colonnes, et les valeurs sont les nouveaux index ou les nouvelles étiquettes des colonnes.\n",
    "\n",
    "Par exemple, renommons la colonne WEEK_END_DATE en END_DATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'WEEK_END_DATE': 'END_DATE'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme autre exemple, renommmons l'index 0 `'First row'`, puis l'index 1 `'Second row'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(index={0:'First row', 1:'Second row'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combinaison\n",
    "Il existe plusieurs méthodes pour joindre deux `DataFrame` ensemble: `concat()`, `join()` et `merge()`. Pourtant, avec seulement `concat()` et `join()`, il est possible de presque tout faire.\n",
    "\n",
    "- `concat()` est utile lorsque vous avez deux `DataFrame` qui contiennent les mêmes colonnes, mais des lignes différentes. En utilisant `concat()`, il est possible de mettre un `DataFrame` à la fin de l'autre (c.-à-d., en bas de l'autre) et d'obtenir ainsi un seul grand `DataFrame`.\n",
    "\n",
    "- `join()` est utile pour joindre des données de deux `DataFrame` qui ont un index en commun. Ceci est quelque peu similaire à la fonction RECHERCHEV dans Excel. La méthode `join()` est souvent utilisée car les données sont souvent dispersées dans plusieurs bases de données.\n",
    "\n",
    "Faisons maintenant un exemple de la méthode `join()`. Supposons que nous avons d'autres données décrivant nos UPC. Par exemple, disons que nous avons le poids (WEIGHT) de chaque UPC dans un autre `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\n",
    "    'UPC': [1111085319, 1111085350, 1600027527, 1600027528, 1600027564, 3000006340, 3800031829],\n",
    "    'WEIGHT': [500, 450, 300, 475, 550, 380, 525]})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour pouvoir amener la variable WEIGHT dans le `DataFrame` principal, nous devrons d'abord définir l'index des deux `DataFrame` avec la colonne UPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = df2.set_index('UPC')\n",
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df.set_index('UPC')\n",
    "left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est maintenant possible de joindre les deux `DataFrame` comme suit (puis de réinitialiser l'index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = left.join(right).reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez cependant que l'ordre a changé. Trions donc les valeurs par WEEK_END_DATE, puis par UPC. Nous réinitialisons à nouveau l'index et supprimons l'ancien index par la suite (`drop=True`) car il n'est pas utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['WEEK_END_DATE', 'UPC']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation des données\n",
    "\n",
    "Différentes façons sont disponibles dans pandas afin de visualiser les données. Il suffit simplement d'utiliser un argument différent dans la méthode `.plot()` comme:\n",
    " - `'bar'` ou `'barh'` pour un diagramme à barre,\n",
    " - `'hist'` pour un histogramme,\n",
    " - `'box'` pour un boxplot,\n",
    " - `'kde'` ou `'density'` pour un diagramme de densité,\n",
    " - `'area'` pour un diagramme de surface,\n",
    " - `'scatter'` pour un nuage de points,\n",
    " - `'hexbin'` pour un diagramme hexagonal, et\n",
    " - `'pie'` pour un diagramme circulaire.\n",
    " \n",
    "Vous pouvez consulter [cette page](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html) pour plus d'informations.\n",
    "\n",
    "À titre d'exemple, visualisons le nombre d'unités (colonne `'UNITS'`). Nous utilisons ici des doubles crochets `[[]]` pour indiquer les colonnes que nous voulons visualiser. Un tracé linéaire est l'option par défaut de la méthode `.plot()`.\n",
    "\n",
    "Dans le code suivant, nous choisissons un produit à tracer. Puisque l'indice sera sur l'axe des x, nous utilisons `WEEK_END_DATE` comme axe des x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.UPC == 1111085319\n",
    "single_prod_df = df.loc[mask][['UNITS', 'VISITS', 'WEEK_END_DATE']]\n",
    "# on peut mettre 'WEEK_END_DATE' comme index\n",
    "# noter que pour un produit les 'WEEK_END_DATE' sont tous différents\n",
    "single_prod_df = single_prod_df.set_index('WEEK_END_DATE')\n",
    "single_prod_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il est aussi possible de changer la taille du graphique\n",
    "single_prod_df.plot(figsize=(12,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut aussi tracer seulement une Series\n",
    "single_prod_df.UNITS.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons combiner certaines des fonctions et méthodes des objets `DataFrame` pour visualiser des informations importantes. Par exemple, si nous sommes intéressés à visualiser le nombre d'unités vendues de chaque produit (`'UPC'`), nous pouvons le faire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupby(['UPC']).sum() \n",
    "df2.plot(y='UNITS', kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou si nous sommes intéressés par la visualisation du prix moyen et du prix de base pour chaque produit, nous pouvons le faire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.groupby(['UPC']).mean()\n",
    "df3.plot(y=['PRICE', 'BASE_PRICE'], kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
