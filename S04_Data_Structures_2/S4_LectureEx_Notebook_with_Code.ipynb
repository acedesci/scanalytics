{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "S4_LectureEx_Notebook_with_Code.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acedesci/scanalytics/blob/master/S04_Data_Structures_2/S4_LectureEx_Notebook_with_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUcH8CXOCFTM"
      },
      "source": [
        "# S4 - Python Data Structures II - DataFrame examples (with code)\n",
        "Programming topics covered in this section:\n",
        "* Creating `DataFrame` and `Series` objects\n",
        "* Reading and writing data using pandas\n",
        "* Indexing, selecting and assigning\n",
        "* Summary functions and operations\n",
        "* Grouping and sorting\n",
        "* Data types and missing values\n",
        "* Renaming and combining\n",
        "\n",
        "Examples include:\n",
        "* Importing and analyzing data of cereal sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP_pgEDNCFTP"
      },
      "source": [
        "----\n",
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBaYzxpGCFTQ"
      },
      "source": [
        "**pandas** is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. This notebook provides a summary of the different components you have seen during the [Kaggle Pandas course](https://www.kaggle.com/learn/pandas). \n",
        "\n",
        "There are two core objects in pandas: \n",
        "* `Series`: A Series is a sequence of indexde values. This is similar to a dictionary. However, Series is the simple form of DataFrame which is much more flexible and pandas offers a large number of pre-built functions and methods for this. You can think about a Series as a DataFrame with a single column. \n",
        "* `DataFrame`: A DataFrame is a data table (similar to a simple data table in Excel with rows and columns). It contains a set of columns which share the same set of indexes. Each column provides the values which correspond to the indexes in the index list.\n",
        "\n",
        "**Note that most operations in pandas are returning you a new object. These operations are not generally done \"in-place\". Thus, if you want to keep your results, you need to assign them to a variable (examples will be provided in this notebook).**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXG0KMTCCFTQ"
      },
      "source": [
        "---\n",
        "## 1. Creating `DataFrame` and `Series` objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phkymq0UCFTR"
      },
      "source": [
        "To use the pandas library, you first need to import it; you can and should import it only one time per Jupyter Notebook. You can do this using different ways (as also for other libraries). The first approach consists of the following:\n",
        "``` python\n",
        "import pandas\n",
        "```\n",
        "By doing so, you then need to use `pandas` in front of all the pandas elements you want to access. For example, to create a `DataFrame`, you would need to do the following:\n",
        "``` python\n",
        "pandas.DataFrame({'Yes':[50, 21], 'No':[131, 2]})\n",
        "```\n",
        "---\n",
        "The second approach consists of importing the library under an alias (i.e., a different name). A convention when importing pandas is to import it under the name `pd`. This allows to reduce the number of letters to type afterwards. For example, the previous code would reduce to:\n",
        "``` python\n",
        "import pandas as pd\n",
        "pd.DataFrame({'Yes':[50, 21], 'No':[131, 2]})\n",
        "```\n",
        "---\n",
        "The third approach consists of importing only the elements you need from the library. For example, if we just need the `DataFrame` object, we could do the following:\n",
        "``` python\n",
        "from pandas import DataFrame\n",
        "DataFrame({'Yes':[50, 21], 'No':[131, 2]})\n",
        "```\n",
        "---\n",
        "Ok, let's stick with the convention and import `pandas` with the following (**NOTE**: this step is important as we need to import pandas before using it!):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxHvYC8CCFTS"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNTqFp8lCFTT"
      },
      "source": [
        "### `Series`\n",
        "It is possible to create a `Series` by providing a list. A `Series` can be considered as one column of a `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAxDH4j9CFTT"
      },
      "source": [
        "pd.Series([4, 5, 2, 9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cZa_AouCFTS"
      },
      "source": [
        "### `DataFrame`\n",
        "The two main objects of interest in the `pandas` library are the `DataFrame` and `Series` objects. It is possible to create a `DataFrame` by passing a dictionnary where the keys are the column labels and the values are lists containing the different elements of the corresponding column. Note that all lists should be of the same length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnqI0HbqCFTS"
      },
      "source": [
        "pd.DataFrame({'Yes':[50, 21], 'No':[131, 2]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S6bxcFnCFTU"
      },
      "source": [
        "---\n",
        "## 2. Reading and writing data using pandas\n",
        "\n",
        "### Importing Data\n",
        "One common use of pandas is to import the data from a file containing a data table (which can be prepared in Excel). \n",
        "\n",
        "**Importing file in Jupyter Notebook**: This can be done using the commands `read_csv()` for CSV (Comma-Separated Values) files or `read_excel()` for Excel files. Other options are also available in the pandas documentation.\n",
        "\n",
        "**If you use Jupyter Notebook**, you can run the code below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMFzVyu3CFTU"
      },
      "source": [
        "# Import the column WEEK_END_DATE as dates\n",
        "df = pd.read_csv('salesCerealsOriginal.csv', parse_dates=['WEEK_END_DATE']) \n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Y6mrzp4LfB"
      },
      "source": [
        "**Importing file in Colab Notebook**: Since Colab is on the cloud, one simple way is to upload it directly. This can be done using the code below **if you use Colab** and then click on \"Choose Files\" to upload it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe0XB34f4I8q"
      },
      "source": [
        "from google.colab import files\r\n",
        "import io \r\n",
        "\r\n",
        "uploaded = files.upload()\r\n",
        "df = pd.read_csv(io.BytesIO(uploaded['salesCerealsOriginal.csv']), parse_dates=['WEEK_END_DATE']) \r\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXLEWRUE6A28"
      },
      "source": [
        "We can quickly see an overview of the data using the function `.head()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZebkRs0CFTU"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBAEsRQWS3HV"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4mOw-qiCFTV"
      },
      "source": [
        "\n",
        "**Note:**\n",
        "* `.read_csv()`: is an important pandas function to read csv files and do operations on it\n",
        "* `parse_dates`: is a parameter of the `.read_csv()` function. It converts the specified data in datetime datatype. Check [this page](https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.read_csv.html) for more information about the parameters of the `.read_csv()` function\n",
        "* `df.shape`: Return a tuple representing the dimensionality of the DataFrame `df`\n",
        "* `.head()`: This function returns the first `n` rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it. By default, `n=5`, so that `df.head()` returns only the first 5 rows of the CSV file read and assigned to `df` \n",
        "\n",
        "Here is a description of the variables in the previous `DataFrame`. This file will also be reused in subsequent sessions.\n",
        "\n",
        "| VARIABLE NAME | DESCRIPTION | \n",
        "|:----|:----|\n",
        "|WEEK_END_DATE|week ending date|\n",
        "|STORE_NUM|store number|\n",
        "|UPC|(Universal Product Code) product specific identifier|\n",
        "|UNITS|units sold|\n",
        "|VISITS|number of unique purchases (baskets) that included the product|\n",
        "|HHS|# of purchasing households|\n",
        "|SPEND|total spend (i.e., $ sales)|\n",
        "|PRICE|actual amount charged for the product at shelf|\n",
        "|BASE_PRICE|base price of item|\n",
        "|FEATURE|product was in in-store circular|\n",
        "|DISPLAY|product was a part of in-store promotional display|\n",
        "|TPR_ONLY|temporary price reduction only (i.e., shelf tag only, product was reduced in price but not on display or in an advertisement)|\n",
        "|DESCRIPTION|product description|\n",
        "|CATEGORY|category of product|\n",
        "|SUB_CATEGORY|sub-category of product|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFsLP_dDCFTV"
      },
      "source": [
        "### Note: Exporting Data\n",
        "**In Jupyter Notebook**: Exporting data can be done with the methods such as `to_csv()` and `to_excel()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiccqLsb6pR8"
      },
      "source": [
        "df.to_csv(\"downloaded_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o68C7sdg6qHZ"
      },
      "source": [
        "**In Colab**: After running the block above, you can download using the following function of the module `files`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulLFzf6d6qW2"
      },
      "source": [
        "files.download(\"downloaded_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-V7ZbOiCFTW"
      },
      "source": [
        "## 3. Indexing, Selecting and Assigning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XrKv9JmCFTW"
      },
      "source": [
        "You can access any columns of a `DataFrame` by either using the dot-notation or by using square brackets `[]`. Then, you can access a specific row within this column by using the brackets `[]` again with the row number. Note that this second indexing can sometime lead to strange results depending on how the indexes are set.\n",
        "\n",
        "Let's take a look to the UPC column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qH20HCECFTW"
      },
      "source": [
        "df.UPC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5x7y6tMCFTW"
      },
      "source": [
        "df.UPC[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKA2mHWICFTX"
      },
      "source": [
        "df['UPC'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPNg3PEeCFTX"
      },
      "source": [
        "Note that we can use either `df.UPC[0]` or `df['UPC'][0]` to access the  Universal Product Code of the first item in the list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoVBVE2dCFTX"
      },
      "source": [
        "### Index-based selection\n",
        "Another way to access elements of a `DataFrame` is by using **index-based selection**, i.e., the `iloc[row_number, col_number]` method. For example, to access the first row of the column number 3 (i.e., the column 'UPC'), we do"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_1ylvqHCFTX"
      },
      "source": [
        "df.iloc[0,2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY3FasJZCFTY"
      },
      "source": [
        "### Label-based selection\n",
        "A third option consists of **label-based selection** where we selects by the row and column labels, i.e., the `.at[]` and `.loc[]` methods. Continuing on our last example, we do"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPYlBn5A7nLT"
      },
      "source": [
        "If you want to access a *single* cell using the index and column names, you can also use `.at[index_name, col_name]` as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULlpOWM07uyF"
      },
      "source": [
        "df.at[0, 'UPC']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTFGfwqQ8xnE"
      },
      "source": [
        "Alternatively, we can also use `.loc[]` as follows. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Pa68P8CFTY"
      },
      "source": [
        "df.loc[0,'UPC']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWx0hMrBCFTY"
      },
      "source": [
        "Note that the difference between `loc[]` and `at[]` is that `.loc[]` is much more flexible and we can use it to obtain the values from more than one cell. In this case, we can use the `loc[]` method with conditional statements (boolean masks). For example, to show all the rows related to the UPC 1111085319, we can do the following. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rbcVqHKCFTY"
      },
      "source": [
        "df.loc[df.UPC == 1111085319]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qlmdPztCFTZ"
      },
      "source": [
        "These boolean expressions can be mixed with the symbol `&` to denote an element-wise 'and', and the symbol `|` to denote an element-wise 'or'. Please note that we cannot directly use the terms `and` and `or` as pandas accepts only the symbols for boolean expressions (see [link](https://pandas.pydata.org/pandas-docs/version/0.15.2/indexing.html#boolean-indexing))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1w-6mi0CFTZ"
      },
      "source": [
        "df.loc[(df.UPC == 1111085319) & (df.FEATURE == True)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utLbZF0iCArp"
      },
      "source": [
        "### Selecting a subset of columns and rows\r\n",
        "We can also use `.loc[row_range, [col_names]]` to slice a subset of rows and columns. We can also assign this to a new DataFrame object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l-BIh5HDm_b"
      },
      "source": [
        "df_col_row_subset = df.loc[:5,['UPC','UNITS']]\r\n",
        "df_col_row_subset # display the result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDEy6v2xBOcU"
      },
      "source": [
        "We can also use double brackets `[[]]` to indicate the columns we want to select (this will select all rows)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYsjxuy-BtEC"
      },
      "source": [
        "df_col_subset = df[['UPC','UNITS']]\r\n",
        "df_col_subset # display the new object of DataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZXX93EhCFTa"
      },
      "source": [
        "## 4. Summary Functions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uUXQXW4CFTa"
      },
      "source": [
        "Some interesting summary functions are the `describe()`, `unique()` and `value_counts()` methods. By running these methods below, we are able to find that there is only one store in this dataset and that only 7 different UPCs are in the dataset. We also find that there is data for 156 weeks.\n",
        "These functions are very useful to do a preliminary analysis of our data.\n",
        "\n",
        "* `describe()`: This method generates a high-level summary of the attributes of the given column. It is type-aware, meaning that its output changes based on the data type of the input (string or numerical)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUhzRl1MCFTb"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Njh7ADCFTb"
      },
      "source": [
        "* `unique()`: shows a list of unique values\r\n",
        "\r\n",
        "NOTE: `df.COLUMN_NAME` can also be used when selecting a *single* column. However, you have to ensure that the column name does not contain any space or special character. Otherwise, you can still use `df[['col_name']]` for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "RE03luTFCFTb"
      },
      "source": [
        "df.STORE_NUM.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YcnfkUbCFTb"
      },
      "source": [
        "* `.value_counts()`: shows a list of unique values and how often they occur in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3YPNSYgCFTc"
      },
      "source": [
        "df.UPC.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH9Yc-8FCFTc"
      },
      "source": [
        "df.WEEK_END_DATE.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-aofXzEJAus"
      },
      "source": [
        "## Computing values for a new column\r\n",
        "\r\n",
        "We can also apply standard calculations to all the elements in a column and add the results to a new column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rlgo7YqFCFTg"
      },
      "source": [
        "df['zNormSPEND'] = (df.SPEND - df.SPEND.mean()) / df.SPEND.std()\n",
        "df['zNormSPEND']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gnrM4OTCFTg"
      },
      "source": [
        "---\n",
        "### Apply\n",
        "The `apply()` method allows to go through each row or each column. You can review more details here: [link](https://www.w3resource.com/pandas/dataframe/dataframe-apply.php) (NOTE: this function is not a basic one but quite useful in practice. It is fine if you do not fully understand it).\n",
        "\n",
        "As an example, let's compute the price rebate in percentage $\\left(\\frac{\\mathit{BASE\\_PRICE} - \\mathit{PRICE}}{\\mathit{BASE\\_PRICE}}\\right) $ when the item is in the circular ($\\mathit{FEATURE}$  is equal to 1). We then add this computation to the column `REBATE_PERC`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVHvoJ6KCFTg"
      },
      "source": [
        "def rebate_perc(row):\n",
        "    if row.FEATURE == 1:\n",
        "        rebate = (row.BASE_PRICE - row.PRICE) / row.BASE_PRICE\n",
        "        return rebate\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "# axis='columns' allows to go through each row\n",
        "# axis='index' allows to go through each column\n",
        "df['REBATE_PERC'] = df.apply(rebate_perc, axis='columns')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZG-9HSTKFlX"
      },
      "source": [
        "Alternatively, we can also use **list comprehension** to calculate each element iteratively as well (If you use this option, please make sure you correctly generate the elements for all rows). The code below is equivalent to the one above but we use list comprehension. This option is not recommended for complex operations as the code will be difficult to read."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ootRcOA5KZ1Q"
      },
      "source": [
        "df['REBATE_PERC_V2'] = [(df.at[i,'BASE_PRICE'] - df.at[i,'PRICE']) / df.at[i,'BASE_PRICE'] \\\r\n",
        "                        if df.at[i,'FEATURE'] == 1 else 0 for i in df.index]\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b09t1d8CFTh"
      },
      "source": [
        "---\n",
        "## 5. Grouping and Sorting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFQdBu9rCFTh"
      },
      "source": [
        "### Group by\n",
        "The `groupby()` method is similar to the PivotTables in Excel. It allows the group the items by one or several dimensions (the Rows and Columns area in the PivotTable). After grouping the items, you can then compute some summary functions over these groups (the Values area in the PivotTable). These summary functions can be your own defined by a `lambda` expression or a standard function definition (using `def ...`).\n",
        "\n",
        "As a first example, let's compute the mean selling price of each `UPC` over the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13G7g_guCFTh"
      },
      "source": [
        "df.groupby('UPC').PRICE.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgsGuXtXCFTh"
      },
      "source": [
        "As a second example, let's compute the number of times that each `UPC` appeared in the circular. Remember that this variable is a binary variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJbyQzfJCFTi"
      },
      "source": [
        "df.groupby('UPC').FEATURE.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-GX7vwaCFTi"
      },
      "source": [
        "### Sort\n",
        "It is also possible to sort `DataFrame` or `Series` by one or multiple variables by using the `sort_values()` method. If you want to instead sort the index, you need to use the `sort_index()` method.\n",
        "\n",
        "For example, below, we are sorting first by `PRICE` and, if there are any equalities, we then sort by `BASE_PRICE`. This sorting is done in descending order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l-_QiYVCFTj"
      },
      "source": [
        "df.sort_values(by=['PRICE', 'BASE_PRICE'], ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXWMC2fiCFTj"
      },
      "source": [
        "## 6.  Data Types and Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9j3rrz0CFTj"
      },
      "source": [
        "Pandas assigns different types to the different columns. Some common types are:\n",
        "- `int` denoted by `int64`\n",
        "- `float` denoted by `float64`\n",
        "- `str` denoted by `object`\n",
        "- dates denoted by `datetime64[ns]`\n",
        "\n",
        "Pandas may denote your dates as `object` if it doesn't understand that these are dates. It doesn't make a big difference unless you want to use some of the nice functions available in pandas to manipulate dates.\n",
        "\n",
        "It is possible to check the type of a column by using the `dtype` property. It is also possible to check the type of all columns by using the `dtypes` property as below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yanGzEu4CFTj"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seMh7qmKCFTk"
      },
      "source": [
        "If you want to convert the type of a column, you can use the `astype()` method.\n",
        "\n",
        "For example, if we want to convert the type of `STORE_NUM` from `int64` to `float64`, we do the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJUxT9vfCFTk"
      },
      "source": [
        "df.STORE_NUM.astype('float64')  # or just use float"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEHNPLroCFTk"
      },
      "source": [
        "### Missing data\n",
        "It is possible to check whether there are null values in a column of a `DataFrame` (or a full `DataFrame`) by using the `isnull()` function (or its companion `notnull()`). These returns a binary mask indicating whether any null values (denoted by `NaN`) are present. Note that `NaN` values are always of type `float64`.\n",
        "\n",
        "Let's check below if we have any missing values in the column `SPEND`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s3Jj0OMCFTk"
      },
      "source": [
        "pd.isnull(df.SPEND)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FEpBBLsCFTl"
      },
      "source": [
        "Let's now check if there are any missing values in any columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNmPjUCGCFTl"
      },
      "source": [
        "pd.isnull(df).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI9AQrIxNucQ"
      },
      "source": [
        "If we had found any missing values, we could have replaced them by using the `fillna()` method. It is also possible to replace other values by using the `replace()` method.\r\n",
        "\r\n",
        "Since the table does not contain any null value, we first manually add a new column with no values (which will automatically contains `None`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FB8C3nWf_89d"
      },
      "source": [
        "df['NewColumn'] = None\r\n",
        "df['NewColumn'] # display this column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cENmHoCYCFTa"
      },
      "source": [
        "df[['UPC', 'UNITS', 'NewColumn']].isnull() # the new column return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBhwEn5I-8I_"
      },
      "source": [
        "If you want to replace such values by something else, you can do so by simply calling the method `fillna()`. Note that if you want to keep the results, you need to assign it to a new variable (which can be the same variable of this original DataFrame object if you want to replace it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_ef3-sW-7Lw"
      },
      "source": [
        "df = df.fillna('Unavailable')\r\n",
        "df[['NewColumn']] # the values would now change"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcL5fqCNCFTl"
      },
      "source": [
        "Alternatively, you can also replace some values as specified. For example, if you want to replace the `STORE_NUM` 367 with some text, you can also use. (Note here that there is only one store number in the data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P4qx4DKCFTl"
      },
      "source": [
        "df['STORE_TXT'] = df.STORE_NUM.replace(367, 'Kwik-E-Mart')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S17JWfU-CFTm"
      },
      "source": [
        "---\n",
        "## 7. Renaming and Combining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGvuAHu4CFTm"
      },
      "source": [
        "It is also possible to rename indexes or columns by using the `rename()` method. An elegant way to use this method is by providing a dictionnary where the keys are the indexes/columns current labels and the values are the indexes/columns new labels.\n",
        "\n",
        "For example, let's rename the `WEEK_END_DATE` column to `END_DATE`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSoyzshLCFTm"
      },
      "source": [
        "df.rename(columns={'WEEK_END_DATE': 'END_DATE'}) \r\n",
        "# Note that the resulting DataFrame is not assigned back to the original one if we don't indicate df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1wYlTvzCFTm"
      },
      "source": [
        "As another example, let's rename the index 0 to 'First row' and then the index 1 to 'Second row'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNeSbJdjCFTm"
      },
      "source": [
        "df.rename(index={0:'First row', 1:'Second row'})\r\n",
        "# Note that the resulting DataFrame is not assigned back to the original one if we don't indicate df = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL9JgvleCFTp"
      },
      "source": [
        "---\n",
        "## 8. Plotting data\n",
        "Plotting methods allow for the default line plot a handful of plot styles. These methods can be provided as the kind keyword argument to `plot()`, and include:\n",
        "\n",
        "* `‘bar’` or `‘barh’` for bar plots\n",
        "* `‘hist’` for histogram\n",
        "* `‘box’` for boxplot\n",
        "* `‘kde’` or `‘density’` for density plots\n",
        "* `‘area’` for area plots\n",
        "* `‘scatter’` for scatter plots\n",
        "* `‘hexbin’` for hexagonal bin plots\n",
        "* `‘pie’` for pie plots\n",
        "\n",
        "You can check [this page](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html) for more information.\n",
        "\n",
        "As an example, let's visualize the number of units (`'UNITS'` column in our data frame). As `.plot()` is a method of `DataFrame` objects, we use double brackets `[[]]` to indicate the columns we want to visualize. A line plot is the default option of the `.plot()` method.\n",
        "\n",
        "In the following code, we choose on the the `UNITS` of one product to be plotted. Since index will be on x-axis, we use `WEEK_END_DATE` to be the x-axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MWClCzq5CFTp"
      },
      "source": [
        "single_prod_df = df.loc[(df.UPC == 1111085319)][['UNITS', 'VISITS', 'WEEK_END_DATE']]\r\n",
        "# we can set the index to be 'WEEK_END_DATE' (note that there is no duplicated value of 'WEEK_END_DATE' for each product)\r\n",
        "single_prod_df = single_prod_df.set_index('WEEK_END_DATE')\r\n",
        "single_prod_df.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw5sRUNcVgOr"
      },
      "source": [
        "# we can also change the size of the plot\r\n",
        "single_prod_df.plot(figsize=(12,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GJEC9R6rCFTq"
      },
      "source": [
        "# We can also plot a single series by using:\r\n",
        "single_prod_df.UNITS.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THA3AbFbCFTq"
      },
      "source": [
        "We can combine some of the functions and methods of `DataFrame` objects to visualize mininful information. For instance, if we are interested in visualizing the number of sold units of each product (`'UPC'`), we can do:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pqlgrLg-CFTr"
      },
      "source": [
        "df2 = df.groupby(['UPC']).sum() \n",
        "df2.plot(y='UNITS', kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxxIuLiCCFTr"
      },
      "source": [
        "Or if we are interested in visualizing the average price and base price for each product, we can do:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSsfON23CFTr"
      },
      "source": [
        "df3 = df.groupby(['UPC']).mean()\n",
        "df3.plot(y=['PRICE', 'BASE_PRICE'], kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}