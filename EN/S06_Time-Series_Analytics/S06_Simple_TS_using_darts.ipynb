{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acedesci/scanalytics/blob/master/EN/S06_Time-Series_Analytics/S06_Simple_TS_using_darts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# S06 Simple time-series forecast and evaluation using darts"
      ],
      "metadata": {
        "id": "nk1RQH9ZoLMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "Time series demand forecasting is crucial for effective supply chain management. With accurate demand forecasts, businesses can optimize inventory, improve demand planning, enhance sales forecasting, and mitigate supply chain risks.\n",
        "\n",
        "With recent development in Python and opensources, there are many simple-to-use packages such as [statsmodels](https://www.statsmodels.org/stable/index.html) (mainly for statistical techniques), [Prophet (by Facebook)](https://facebook.github.io/prophet/), [GluonTS (by Amazon)] (https://ts.gluon.ai/stable/), and many other libraries. Most libraries require a specific (but similar) data input format and processes. Thus, the most important step to use such packages is to prepare the data in the right format.\n",
        "\n",
        "There are also opensource libaries that are built upon many time-series forecasting packages and provide interfaces to many time-series algorithms such as [sktime](https://www.sktime.net/en/latest/index.html) and [darts](https://unit8co.github.io/darts/). These time-series forecasting interfaces greatly simplify forecasting with its intuitive API and diverse range of models, including classical statistical methods and modern machine learning approaches including deep-learning-based models.\n",
        "\n",
        "In this notebook, we will provide a simple walkthrough to the time-series analysis using darts. There is also a demo that provides more comprehensive pipelines of the forecasting processes."
      ],
      "metadata": {
        "id": "5YfdPZHsh0_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, an installation of `dart` is mandatory since it is not included in Colab by default."
      ],
      "metadata": {
        "id": "Kw2VhJBniX9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The one installs the full version of darts.\n",
        "# If it causes an error showing \"numpy\", please click \"Runtime -> Restart session\" and then run the notebook again\n",
        "!pip install darts\n",
        "!pip install statsforecast\n",
        "\n",
        "# Install missing dependencies\n",
        "!pip install pytorch-lightning\n",
        "\n",
        "# this one is another installation option that might work\n",
        "# !pip install \"u8darts[torch]\""
      ],
      "metadata": {
        "id": "tHBIRFGLiV15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1**: Load data\n",
        "\n",
        "In order to use `darts`, it requires a specific object called `TimeSeries`. This is basically very similar to `DataFrame` in the time-series format and we can simply pass the time-series in the Series or DataFrame format to create the `TimeSeries` object for `darts`. **IMPORTANT**: The index of the Series or DataFrame prior to conversion must be in `datetime` format (i.e., using the function `pd.to_datetime(...)`)"
      ],
      "metadata": {
        "id": "yacXcJKQjYKp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eXSlmoYhhmF"
      },
      "outputs": [],
      "source": [
        "# simple Python code for time-series using darts\n",
        "\n",
        "import pandas as pd\n",
        "import darts\n",
        "\n",
        "data = pd.read_csv('https://bit.ly/m5simple', index_col='ds')\n",
        "data.index = pd.to_datetime(data.index)\n",
        "\n",
        "y_timeseries = darts.TimeSeries.from_series(data['y'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2**: Train/test split\n",
        "\n",
        "We then split the time-series data into the training set and test set. The test set contains the last 52 data points whereas the training set contains all the data from the beginning until the period preceeding the test set."
      ],
      "metadata": {
        "id": "JpfEC3l2jHvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_n_points = 52\n",
        "\n",
        "start = len(data)-test_n_points\n",
        "train, test = y_timeseries.split_before(start)"
      ],
      "metadata": {
        "id": "Gpjx1GpxjE1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3**: Train the model\n",
        "\n",
        "Similar to `sklearn`, we can simply create a model object and fit the data to the model. In the following block, we provide examples of different time-series models and you can choose one to try. Then the function `.fit(...)` is used to fit the data to the model (training).\n",
        "\n",
        "The list of the models supported by `darts` is available here:\n",
        "https://unit8co.github.io/darts/generated_api/darts.models.forecasting.html"
      ],
      "metadata": {
        "id": "hbaMYfRsj9SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from darts.models import ExponentialSmoothing, AutoARIMA, Theta, Prophet, Croston\n",
        "\n",
        "model = ExponentialSmoothing()\n",
        "# model = AutoARIMA()\n",
        "# model = Theta()\n",
        "# model = Prophet()\n",
        "# model = Croston()\n",
        "\n",
        "# Uncomment below if you want to try the deep learning NBEAT model (there could be a dependency issue)\n",
        "# from darts.models import NBEATSModel\n",
        "# model = NBEATSModel(input_chunk_length=52, output_chunk_length=52, n_epochs=50)\n",
        "\n",
        "# Uncomment below if you want to try LightGBMModel\n",
        "# from darts.models import LightGBMModel\n",
        "# model = LightGBMModel(lags=52)\n",
        "\n",
        "# LightGBM is a gradient boosting framework that uses tree-based learning algorithms.\n",
        "# When using LightGBM with Darts, you often need to specify 'lgbm_kwargs' to pass parameters\n",
        "# directly to the LightGBM regressor.\n",
        "# 'lgbm_kwargs' can include parameters like 'n_estimators', 'learning_rate', 'num_leaves', etc.\n",
        "# For time series, it's crucial to also provide 'lags' to specify which past values of the series\n",
        "# should be used as features.\n",
        "\n",
        "model.fit(train)"
      ],
      "metadata": {
        "id": "wFXdvYOPituc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4**: Create predictions for the period of the test set.\n",
        "\n",
        "We can then generate the prediction of the following 52 periods, which corresponds to the period in the test set in order to measure the quality of the forecasts on an out-of-sample basis (using test data that has not been included in the training set). We can also convert the output into series (by using `.pd_series()`) or dataframe (by using `.pd_dataframe()`)."
      ],
      "metadata": {
        "id": "bCXiEnVUkfOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forecast = model.predict(len(test))\n",
        "\n",
        "forecast.to_series()"
      ],
      "metadata": {
        "id": "91hJcvpyka8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5**: Measure the forecasting errors.\n",
        "\n",
        "We measure the results using five different error measures, i.e., Mean Absolute Percentage Error (MAPE) using the function `mape()`, Symmetric Mean Absolute Percentage Error (sMAPE)using the function `smape()`, Root Mean Squared Scaled Error (RMSSE) using the function `rmsse()`, Root Mean Squared Error (RMSE) using the function `rmse()`, and Mean Error (ME) using the function `merr()`."
      ],
      "metadata": {
        "id": "wkR3gtsDlQEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m_mape = darts.metrics.mape(test, forecast)\n",
        "m_wmape = darts.metrics.wmape(test, forecast)\n",
        "m_rmsse = darts.metrics.rmsse(test, forecast, insample = train)\n",
        "m_rmse = darts.metrics.rmse(test, forecast)\n",
        "m_merr = darts.metrics.merr(test, forecast)\n",
        "\n",
        "\n",
        "print(f\"The model obtains Mean Absolute Percentage Error: {m_mape:.2f}%\")\n",
        "print(f\"The model obtains Weighted Mean Absolute Percentage Error: {m_wmape:.2f}%\")\n",
        "print(f\"The model obtains Root Mean Squared Scaled Error: {m_rmsse:.2f}\")\n",
        "print(f\"The model obtains Root Mean Squared Error: {m_rmse:.2f}\")\n",
        "print(f\"The model obtains Mean Error: {m_merr:.2f}\")"
      ],
      "metadata": {
        "id": "4pXPWk6_lPeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also plot the results using `seaborn`"
      ],
      "metadata": {
        "id": "YMiMzrzsnben"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(18,6))\n",
        "\n",
        "sns.scatterplot(x = data[-104:].index, y = data['y'][-104:].values, label = 'true')\n",
        "sns.lineplot(data = forecast.to_series(), label = 'Forecast')"
      ],
      "metadata": {
        "id": "1U5FitiUnXCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a26e3897"
      },
      "source": [
        "### Note: Understanding LightGBM's Tabular Input\n",
        "\n",
        "When `LightGBMModel` in `darts` is used with a `lags` parameter, it implicitly converts the time series data into a tabular format where each row represents a time step, and columns represent the target variable and its lagged values (features). For `lags=52`, it means that for each prediction point, the model considers the previous 52 values of the series as features. Since this process is prebuilt in the package, we cannot fully see the transformed data that is used in the training process.\n",
        "\n",
        "To illustrate this, the following code manually creates a DataFrame with lagged features to illustrate this concept for our `train` dataset. This will show you the kind of tabular data LightGBM works with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceb9aef4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the Darts TimeSeries 'train' to a pandas Series\n",
        "train_series = train.to_series()\n",
        "\n",
        "# Create a DataFrame to store the lagged features\n",
        "lagged_df = pd.DataFrame(index=train_series.index)\n",
        "\n",
        "# Add the target variable (y) to the DataFrame\n",
        "lagged_df['y'] = train_series\n",
        "\n",
        "# Define the number of lags (as used in LightGBMModel)\n",
        "lags_to_generate = 52\n",
        "\n",
        "# Generate lagged features\n",
        "for i in range(1, lags_to_generate + 1):\n",
        "    lagged_df['lag_'+str(i)] = train_series.shift(i)\n",
        "\n",
        "# Display the first few rows of the generated lagged DataFrame\n",
        "# Note: The first 'lags_to_generate' rows will have NaN values for lags,\n",
        "# as there aren't enough preceding data points.\n",
        "display(lagged_df.head(15))\n",
        "display(lagged_df.tail(15))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}