{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBOQyRid2bUiIj8SFVCsPT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acedesci/scanalytics/blob/master/EN/S06_Time-Series_Analytics/Demo_Time_Series_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo: Time-series demand forecasting pipeline"
      ],
      "metadata": {
        "id": "XOVvL--ZvrAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Time series demand forecasting is crucial for effective supply chain management. With accurate demand forecasts, businesses can optimize inventory, improve demand planning, enhance sales forecasting, and mitigate supply chain risks.\n",
        "\n",
        "With recent development in Python and opensources, there are many simple-to-use packages such as [statsmodels](https://www.statsmodels.org/stable/index.html) (mainly for statistical techniques), [Prophet (by Facebook)](https://facebook.github.io/prophet/), [GluonTS (by Amazon)] (https://ts.gluon.ai/stable/), and many other libraries. Most libraries require a specific (but similar) data input format and processes. Thus, the most important step to use such packages is to prepare the data in the right format.\n",
        "\n",
        "There are also opensource libaries that are built upon many time-series forecasting packages and provide interfaces to many time-series algorithms such as [sktime](https://www.sktime.net/en/latest/index.html) and [darts](https://unit8co.github.io/darts/). These time-series forecasting interfaces greatly simplify forecasting with its intuitive API and diverse range of models, including classical statistical methods and modern machine learning approaches including deep-learning-based models. An illustration of a time-series pipeline based on the case [1] is depicted below. Note that, in the case, both time-series-based (which requires time-series data format) and machine-learning-based (which requires tabular data format) models are used simultaneously whereas this notebook focuses on the use of time-series models using time-series data format.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/acedesci/scanalytics/blob/master/EN/S06_Time-Series_Analytics/BBD_forecasts.png?raw=true'/>\n",
        "<figcaption>Forecasting pipeline as depicted in [1]</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "This notebook provide a simple walkthrough for an implementation of time-series forecasting pipeline using `darts`. At the end of the notebook, it also demonstrates time-phased dynamic inventory policy that can be calculated from the forecasts from various forecasting models. The quality of this inventory policy can then be measured through a simulation which has been covered in an earlier session on `DataFrame`.\n",
        "\n",
        "[1] Dodin, P., Xiao, J., Adulyasak, Y., Alamdari, N.E., Gauthier, L., Grangier, P., Lemaitre, P. and Hamilton, W.L., 2023. **Bombardier aftermarket demand forecast with machine learning.** *INFORMS Journal on Applied Analytics*, 53(6), pp.425-445."
      ],
      "metadata": {
        "id": "5GapjXnmwCxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block description**: We first install `darts` (this must be run every time a new colab notebook is open)."
      ],
      "metadata": {
        "id": "22vwdwz52vRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install darts"
      ],
      "metadata": {
        "id": "K1XzY6lC84P_",
        "outputId": "7ee0e903-3ffa-49c5-ae92-8c222797b36f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting darts\n",
            "  Downloading darts-0.32.0-py3-none-any.whl.metadata (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: holidays>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from darts) (0.66)\n",
            "Requirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from darts) (1.4.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from darts) (3.10.0)\n",
            "Collecting nfoursid>=1.0.0 (from darts)\n",
            "  Downloading nfoursid-1.0.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from darts) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from darts) (2.2.2)\n",
            "Collecting pmdarima>=1.8.0 (from darts)\n",
            "  Downloading pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
            "Collecting pyod>=0.9.5 (from darts)\n",
            "  Downloading pyod-2.0.3.tar.gz (169 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.6/169.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2.32.3)\n",
            "Collecting scikit-learn<1.6.0,>=1.0.1 (from darts)\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from darts) (1.13.1)\n",
            "Requirement already satisfied: shap>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from darts) (0.46.0)\n",
            "Collecting statsforecast>=1.4 (from darts)\n",
            "  Downloading statsforecast-2.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: statsmodels>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from darts) (0.14.4)\n",
            "Collecting tbats>=1.1.0 (from darts)\n",
            "  Downloading tbats-1.1.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.11/dist-packages (from darts) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from darts) (4.12.2)\n",
            "Requirement already satisfied: xarray>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2025.1.2)\n",
            "Requirement already satisfied: xgboost>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2.1.3)\n",
            "Collecting pytorch-lightning>=1.5.0 (from darts)\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting tensorboardX>=2.1 (from darts)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2.5.1+cu124)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from holidays>=0.11.1->darts) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (3.2.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->darts) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->darts) (2025.1)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.11/dist-packages (from pmdarima>=1.8.0->darts) (3.0.11)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from pmdarima>=1.8.0->darts) (2.3.0)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.11/dist-packages (from pmdarima>=1.8.0->darts) (75.1.0)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.11/dist-packages (from pyod>=0.9.5->darts) (0.61.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning>=1.5.0->darts) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (2024.10.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning>=1.5.0->darts)\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>=1.5.0->darts)\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->darts) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->darts) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->darts) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.0.1->darts) (3.5.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap>=0.40.0->darts) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap>=0.40.0->darts) (3.1.1)\n",
            "Collecting coreforecast>=0.0.12 (from statsforecast>=1.4->darts)\n",
            "  Downloading coreforecast-0.0.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting fugue>=0.8.1 (from statsforecast>=1.4->darts)\n",
            "  Downloading fugue-0.9.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting utilsforecast>=0.1.4 (from statsforecast>=1.4->darts)\n",
            "  Downloading utilsforecast-0.2.11-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.14.0->darts) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.1->darts) (4.25.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->darts)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->darts) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (3.11.11)\n",
            "Collecting triad>=0.9.7 (from fugue>=0.8.1->statsforecast>=1.4->darts)\n",
            "  Downloading triad-0.9.8-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting adagio>=0.2.4 (from fugue>=0.8.1->statsforecast>=1.4->darts)\n",
            "  Downloading adagio-0.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51->pyod>=0.9.5->darts) (0.44.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->holidays>=0.11.1->darts) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->darts) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.18.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast>=1.4->darts) (17.0.0)\n",
            "Collecting fs (from triad>=0.9.7->fugue>=0.8.1->statsforecast>=1.4->darts)\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting appdirs~=1.4.3 (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast>=1.4->darts)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Downloading darts-0.32.0-py3-none-any.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.3/963.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nfoursid-1.0.1-py3-none-any.whl (16 kB)\n",
            "Downloading pmdarima-2.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsforecast-2.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tbats-1.1.3-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/27/94/3266821f65b92b3138631e9c8e7fe1fb513804ac934485a8d05776e1dd43/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coreforecast-0.0.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (275 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.8/275.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fugue-0.9.1-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
            "Downloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading utilsforecast-0.2.11-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading adagio-0.2.6-py3-none-any.whl (19 kB)\n",
            "Downloading triad-0.9.8-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Building wheels for collected packages: pyod\n",
            "  Building wheel for pyod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyod: filename=pyod-2.0.3-py3-none-any.whl size=200466 sha256=2f0c101228bc3a010fb6287eac2e677fd2b39bb7b3f27477a47f831c71e74d43\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/60/5b/f74eccd2c9c892a2c298202ca510f10995f9940647fcc2d97f\n",
            "Successfully built pyod\n",
            "Installing collected packages: appdirs, tensorboardX, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, fs, coreforecast, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, utilsforecast, triad, pyod, nvidia-cusolver-cu12, nfoursid, pmdarima, adagio, torchmetrics, tbats, fugue, statsforecast, pytorch-lightning, darts\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data inputs\n",
        "\n",
        "This code block primarily  sets up the environment for time series analysis using models available in `darts`.\n",
        "\n",
        "*Notable remarks:*\n",
        "\n",
        "*   `data = pd.read_csv(..., index_col='ds')`: Reads time series data from a CSV file located at the given URL. It assumes the file has a column named 'ds' that represents the dates or timestamps and sets it as the index of the DataFrame.\n",
        "\n",
        "*   `y = data['y']`: Extracts the time series data (likely the demand values) from the 'y' column of the DataFrame and assigns it to the variable y.\n",
        "\n",
        "*   `plot_n_points = 104 `: Sets the number of data points to be used for plotting.\n",
        "\n",
        "*   `test_n_points = 65`: Sets the number of data points to be used for testing the forecasting models."
      ],
      "metadata": {
        "id": "2uSieO9c3AHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import darts\n",
        "from darts.models import AutoARIMA, Prophet, ExponentialSmoothing, Theta, NaiveSeasonal, Croston, NBEATSModel\n",
        "from darts.models import NaiveEnsembleModel, RegressionEnsembleModel\n",
        "from darts.utils.utils import ModelMode, SeasonalityMode\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "from darts.metrics import mape, smape, mase, rmsse, rmse, merr\n",
        "from darts import TimeSeries\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# read historical time-series data (demand)\n",
        "data = pd.read_csv('https://bit.ly/m5simple', index_col='ds')\n",
        "data.index = pd.to_datetime(data.index)\n",
        "n_periods = len(data.index)\n",
        "\n",
        "# obtain the series of time series from column 'y'\n",
        "y = data['y']\n",
        "\n",
        "# number of historical data points to show on the plots (in weeks)\n",
        "plot_n_points = 104\n",
        "\n",
        "# number of data points used as the test set\n",
        "test_n_points = 65\n"
      ],
      "metadata": {
        "id": "tyAN4usQAVfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define key functions\n",
        "\n",
        "This code block defines the following functions that will be called often in the pipeline.\n",
        "\n",
        "**Function** `calculate_test_errors`:\n",
        "\n",
        "This function calculates and return various error metrics for a time series forecast.\n",
        "\n",
        "Input:\n",
        "\n",
        "`test_series`: The actual values of the time series during the testing period.\n",
        "`forecast_series`: The predicted values generated by a forecasting model during the testing period.\n",
        "\n",
        "Process:\n",
        "\n",
        "It uses functions from the darts.metrics module (`mape, smape, rmsse, rmse, merr`) to calculate the following error metrics (see [Darts error metrics](https://unit8co.github.io/darts/generated_api/darts.metrics.html)):\n",
        "\n",
        "\n",
        "* MAPE: Mean Absolute Percentage Error (from function `mape`)\n",
        "* sMAPE: Symmetric Mean Absolute Percentage Error (from function `smape`)\n",
        "* RMSSE: Root Mean Squared Scaled Error (from function `rmsse`)\n",
        "* RMSE: Root Mean Squared Error (from function `rmse`)\n",
        "* ME: Mean Error (from function `merr`)\n",
        "* Norm_ME: Mean Error normalized by the mean of actual demand\n",
        "\n",
        "Output:\n",
        "The function returns the errors DataFrame containing the calculated error metrics for the model.\n",
        "\n",
        "**Function** `add_ranks`:\n",
        "\n",
        "This function is used to add rank columns to a DataFrame based on the values in existing columns, It returns the modified DataFrame with the added rank columns."
      ],
      "metadata": {
        "id": "dQtUdpyeOgk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# functions to return test errors\n",
        "\n",
        "def calculate_test_errors(test_series, forecast_series, series_name = 'y', print_results = False, skip_mape = False):\n",
        "  # get the model name (the series from dart include the name of the model as the name of the series)\n",
        "  m_name = forecast_series.name\n",
        "\n",
        "  # convert the series of test set (actual values) and forecast set (forecasted values) into darts TimeSeries objects\n",
        "  # (since we will use darts methods to calculate the errors)\n",
        "  test = TimeSeries.from_series(test_series)\n",
        "  forecast = TimeSeries.from_series(forecast_series)\n",
        "\n",
        "  # In some cases, we need to skip MAPE if we have a zero demand observation since we cannot divide the error by zero\n",
        "  # this is currently done manually. In any case, darts will also return an error if there is a zero demand when calculating mape.\n",
        "  # The following part calculates all the forecasting errors we want to measure\n",
        "  if not skip_mape:\n",
        "    m_mape = mape(test, forecast)\n",
        "    m_smape = smape(test, forecast)\n",
        "\n",
        "  m_rmsse = rmsse(test, forecast, insample = train)\n",
        "  m_rmse = rmse(test, forecast)\n",
        "  m_merr = merr(test, forecast)\n",
        "\n",
        "  if print_results:\n",
        "    if not skip_mape:\n",
        "      print(f\"model {m_name} obtains Mean Absolute Percentage Error: {m_mape:.2f}%\")\n",
        "      print(f\"model {m_name} obtains symmetric Mean Absolute Percentage Error: {m_smape:.2f}%\")\n",
        "\n",
        "    print(f\"model {m_name} obtains Root Mean Squared Scaled Error: {m_rmsse:.2f}\")\n",
        "    print(f\"model {m_name} obtains Root Mean Squared Error: {m_rmse:.2f}\")\n",
        "    print(f\"model {m_name} obtains Mean Error: {m_merr:.2f}\")\n",
        "\n",
        "  # Now we put together the errors in the dataframe 'errors'=\n",
        "  test_mean = test.pd_series().mean()\n",
        "\n",
        "  if not skip_mape:\n",
        "    column_names=['SeriesName', 'Model', 'MAPE', 'sMAPE', 'RMSSE', 'RMSE', 'Norm_RMSE', 'ME', 'Norm_ME']\n",
        "    errors = pd.DataFrame(columns=column_names)\n",
        "    errors.loc[0] = [series_name, m_name, m_mape, m_smape, m_rmsse, m_rmse, m_rmse/test_mean, m_merr, m_merr/test_mean]\n",
        "  else:\n",
        "    column_names=['SeriesName', 'Model', 'RMSSE', 'RMSE', 'Norm_RMSE', 'ME', 'Norm_ME']\n",
        "    errors = pd.DataFrame(columns=column_names)\n",
        "    errors.loc[0] = [series_name, m_name, m_rmsse, m_rmse, m_rmse/test_mean, m_merr, m_merr/test_mean]\n",
        "\n",
        "  return errors"
      ],
      "metadata": {
        "id": "TP9O_RLsLFyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function takes a DataFrame of the error values and then return the ranking of the results based on the obsolute values\n",
        "# (the lower the value the higher the rank)\n",
        "def add_ranks(df, ascending=True):\n",
        "  for col in df.columns[2:]:\n",
        "    df['Rank_'+col] = df[col].abs().rank(ascending=ascending)\n",
        "  return df"
      ],
      "metadata": {
        "id": "OcTmRiifHGyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline 1: Forecasting and evaluation using two-fold validation\n",
        "\n",
        "The code block calls and evaluates various time series forecasting models in the `darts` library. It first prepares the data, visualizes the actual values, and defines a list of forecasting models, including ensembles. Then, it iteratively trains each model, generates forecasts, calculates error metrics. Finally, it combines and displays the errors and ranks of each model, providing a performance comparison based on different models for the given dataset.\n",
        "\n",
        "The training and evaluation of this pipeline is based on a two-fold validation process where data is split only into one training and one test set at a given split point. The predictions are evaluated against the actual values in the test set (which is not used in the training process).\n"
      ],
      "metadata": {
        "id": "nfVzOJqp5cHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single split (two-fold)\n",
        "\n",
        "# Create a plot the time-series and plot the actual values (both the train and test)\n",
        "plt.figure(figsize=(18,6))\n",
        "sns.scatterplot(x = y[-plot_n_points:].index, y = y[-plot_n_points:].values, label = 'true')\n",
        "\n",
        "# Convert the series y into Darts's TimeSeries object (so that we can use darts methods)\n",
        "y_timeseries = TimeSeries.from_series(y)\n",
        "\n",
        "# We will also try an ensemble model that combines the foorecast of the following, methods\n",
        "ensemble_models = [NaiveSeasonal(K=52), Theta(season_mode = SeasonalityMode.ADDITIVE), Croston()]\n",
        "\n",
        "# Here is the list of the models (all supported by darts) and their corresponding input parameters we will run\n",
        "list_models = [NaiveSeasonal(K=52), \\\n",
        "               ExponentialSmoothing(seasonal = None), \\\n",
        "               AutoARIMA(), \\\n",
        "               Theta(), \\\n",
        "               # Prophet(), \\\n",
        "               # NBEATSModel(input_chunk_length=52, output_chunk_length=52, n_epochs=50), \\\n",
        "               Croston(),\\\n",
        "               NaiveEnsembleModel(forecasting_models=ensemble_models), \\\n",
        "               RegressionEnsembleModel(forecasting_models=ensemble_models, regression_train_n_points=13)]\n",
        "\n",
        "# We will prepare the list of errors that will contain the errors from each model to compare.\n",
        "# This errors will be put together in a dataframe 'pd_errors' later on\n",
        "list_errors = []\n",
        "\n",
        "# We will also prepare a DataFrame that contains all the forecasts\n",
        "df_forecasts = pd.DataFrame()\n",
        "\n",
        "# We first add the first column based on the actual data\n",
        "df_forecasts['y'] = y\n",
        "\n",
        "# 'start' here define the split between the training and test periods\n",
        "start = len(y)-test_n_points\n",
        "# Now we split the train and test at the start point (of the test)\n",
        "train, test = y_timeseries.split_before(start)\n",
        "\n",
        "# Now we perform the forecast using one model in each loop, and calculate the forecasting errors\n",
        "# The results are then added to the DataFrame 'df_forecasts'\n",
        "for model in list_models:\n",
        "  # First, we use the first 15 characters of the name of the model as the name to be displayed to DataFrame\n",
        "  # (some names are too long so we cut only the first 15 characters)\n",
        "  m_name = str(model)[:15]\n",
        "  print(\"Forecast using model: \", m_name)\n",
        "\n",
        "  # Train (fit) the mode using the training set\n",
        "  model.fit(train)\n",
        "\n",
        "  # Predict for the next 'test_n_points' and then convert the outputs (which is Darts TimeSeries object) into a series\n",
        "  forecast_series = model.predict(test_n_points).pd_series()\n",
        "  # Set the name of the series to the name of the model\n",
        "  forecast_series.name = m_name\n",
        "  # Now call the function to calculate all the errors\n",
        "  errors = calculate_test_errors(test.pd_series(), forecast_series, series_name = 'y')\n",
        "  # Add the forecast plot from this model to the time-series plot\n",
        "  sns.lineplot(data = forecast_series, label = forecast_series.name)\n",
        "  # Append the errors (list) into 'list_errors'\n",
        "  list_errors.append(errors)\n",
        "  # Add the forecast to 'df_forecasts' that store the forecasts of all the models\n",
        "  df_forecasts[forecast_series.name] = forecast_series\n",
        "\n",
        "# Once finished, we then convert the list of list of errors into a DataFrame\n",
        "pd_errors = pd.concat(list_errors, ignore_index = True)\n",
        "\n",
        "# We then add the columns to show the rankings based on the performances of the forecasts of different models\n",
        "pd_errors = add_ranks(pd_errors)\n",
        "\n",
        "pd_errors"
      ],
      "metadata": {
        "id": "tvFLbMGD7m9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define rolling forecasts and plot functions\n",
        "\n",
        "**Function** `eval_model_rolling_forecasts`:\n",
        "\n",
        "This function simulates a real-world forecasting scenario where one would retrain the model periodically with new data to make future predictions. It provides a more realistic evaluation of the model's performance over time compared to a single train-test split. This functions requires the **following inputs**:\n",
        "\n",
        "* `model`: darts models (see [darts forecasting models](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.html))\n",
        "* `series`: a series containing the entire time-series data\n",
        "* `start`: the first starting point of the forecasting evaluation\n",
        "* `fh`: length of the forecast horizon\n",
        "* `stride`: the retraining (forecast update) internal (i.e., new forecasts are produced every `stride` periods). This value must be $\\leq$ `fh`\n",
        "\n",
        "The model returns a series containing the predictions that are produced using a rolling update of every `stride` periods. Only the most recent values of the predictions are kept in the series.\n",
        "\n",
        "**Function** `plot_forecasts`:\n",
        "\n",
        " This function is used to visualize time series data from a DataFrame containing actual and forecasted data.It generates a plot with actual values displayed as scatter points and forecasts from different models represented as lines."
      ],
      "metadata": {
        "id": "z-_si9uFQVCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def eval_model_rolling_forecasts(model, series, start, fh, stride, series_name = 'y'):\n",
        "  m_name = str(model)[:15]\n",
        "\n",
        "  timeseries = TimeSeries.from_series(series)\n",
        "  list_hist_rolling_forecasts = []\n",
        "  for starting_point in range(start, len(timeseries), stride):\n",
        "    train, val = timeseries.split_before(starting_point)\n",
        "    forecast_horizon = min(fh, len(val))\n",
        "    forecasts = model.fit(train).predict(forecast_horizon)\n",
        "    list_hist_rolling_forecasts.append(forecasts.pd_series())\n",
        "\n",
        "  last_fold = len(list_hist_rolling_forecasts)-1\n",
        "  hist_rolling_forecasts = list_hist_rolling_forecasts[last_fold]\n",
        "\n",
        "  for fold in range(last_fold):\n",
        "    hist_rolling_forecasts = pd.concat([hist_rolling_forecasts, list_hist_rolling_forecasts[fold][:stride]])\n",
        "\n",
        "  hist_rolling_forecasts.name = m_name\n",
        "  print(\"\\t----- generating rolling forecasts using model \" +str(hist_rolling_forecasts.name))\n",
        "\n",
        "  return hist_rolling_forecasts\n"
      ],
      "metadata": {
        "id": "LzcpyAeqYWVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_forecasts(df_forecasts, n_points, series_name = 'y', plot_name = 'Prediction'):\n",
        "  plt.figure(figsize=(18,6))\n",
        "  plt.title(plot_name+\": \" + series_name)\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Value\")\n",
        "  y = df_forecasts['y']\n",
        "  sns.scatterplot(x = y[-n_points:].index, y = y[-n_points:].values, label = 'true')\n",
        "\n",
        "  for model in df_forecasts.columns[1:]:\n",
        "    sns.lineplot(data = df_forecasts[model][-n_points:], label = model)\n"
      ],
      "metadata": {
        "id": "73rxmhKR8yJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline 2: Forecasting and evaluation using sequential validation\n",
        "\n",
        "This code block is similar to pipeline 1 but it simulates and evaluates the forecasts on a rolling basis using the previously defined function `eval_model_rolling_forecasts(...)`."
      ],
      "metadata": {
        "id": "KfvRhnGx6zBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rolling forecasts evaluation (multifold sequential validation)\n",
        "\n",
        "val_size = test_n_points\n",
        "y_timeseries = TimeSeries.from_series(y)\n",
        "start = len(y)-val_size\n",
        "fh = 13\n",
        "stride=13\n",
        "\n",
        "ensemble_models = [NaiveSeasonal(K=52), Theta(season_mode = SeasonalityMode.ADDITIVE), Croston()]\n",
        "\n",
        "list_models = [NaiveSeasonal(K=52),\n",
        "               ExponentialSmoothing(),\n",
        "               AutoARIMA(), \\\n",
        "               Theta(), \\\n",
        "               # Prophet(), \\\n",
        "               # NBEATSModel(input_chunk_length=52, output_chunk_length=52, n_epochs=50), \\\n",
        "               Croston(),\\\n",
        "               NaiveEnsembleModel(forecasting_models=ensemble_models), \\\n",
        "               RegressionEnsembleModel(forecasting_models=ensemble_models, regression_train_n_points=13)\n",
        "               ]\n",
        "\n",
        "list_errors = []\n",
        "df_forecasts = pd.DataFrame()\n",
        "df_forecasts['y'] = y\n",
        "train, test = y_timeseries.split_before(start)\n",
        "\n",
        "# Now we perform the forecast using one model in each loop, and calculate the forecasting errors\n",
        "# The results are then added to the DataFrame 'df_forecasts'\n",
        "for model in list_models:\n",
        "  forecast_series = eval_model_rolling_forecasts(model, y, start, fh, stride)\n",
        "  errors = calculate_test_errors(test.pd_series(), forecast_series)\n",
        "  list_errors.append(errors)\n",
        "  df_forecasts[forecast_series.name] = forecast_series\n",
        "\n",
        "pd_errors = pd.concat(list_errors)\n",
        "\n",
        "# We also use our previously defined function 'add_ranks' to\n",
        "pd_errors = add_ranks(pd_errors)\n",
        "\n",
        "plot_forecasts(df_forecasts, plot_n_points)\n",
        "\n",
        "pd_errors"
      ],
      "metadata": {
        "id": "SbgfU-0t7lFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline 3: Automating forecasting and sequential validation on multiple items\n",
        "\n",
        "This code block is similar to pipeline 2 that performs rolling forecasts. The code automate the predictions of multiple items through iterative process."
      ],
      "metadata": {
        "id": "Q_pe_lE99cq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline to run forecast for multiple items\n",
        "\n",
        "import time\n",
        "\n",
        "# state-item level\n",
        "url = 'https://raw.githubusercontent.com/acedesci/scanalytics/refs/heads/master/EN/data/M5/processed_data/m5data_CA_item_state_rand.csv'\n",
        "\n",
        "# store-item level (slow-moving)\n",
        "# url = 'https://raw.githubusercontent.com/acedesci/scanalytics/refs/heads/master/EN/data/M5/processed_data/m5data_CA_item_store_n25_cv.csv'\n",
        "\n",
        "data = pd.read_csv(url, index_col='ds')\n",
        "data.index = pd.to_datetime(data.index)\n",
        "\n",
        "list_items = data.columns[:3]\n",
        "print(\"list of items: \" + str(list_items))\n",
        "\n",
        "val_size = test_n_points\n",
        "start = len(data.index)-val_size\n",
        "fh = 13\n",
        "stride = 13\n",
        "\n",
        "ensemble_models = [NaiveSeasonal(K=52), Theta(season_mode = SeasonalityMode.ADDITIVE), Croston()]\n",
        "\n",
        "list_models = [NaiveSeasonal(K=52),\n",
        "               # ExponentialSmoothing(),\n",
        "               AutoARIMA(), \\\n",
        "               Theta(season_mode = SeasonalityMode.ADDITIVE), \\\n",
        "               # Prophet(), \\\n",
        "               # NBEATSModel(input_chunk_length=52, output_chunk_length=52, n_epochs=50), \\\n",
        "               Croston(),\\\n",
        "               NaiveEnsembleModel(forecasting_models=ensemble_models), \\\n",
        "               RegressionEnsembleModel(forecasting_models=ensemble_models, regression_train_n_points=13)\n",
        "               ]\n",
        "\n",
        "pd_all_errors = pd.DataFrame()\n",
        "\n",
        "list_forecasts = []\n",
        "\n",
        "# Iterate through each item (time-series) in the list of items\n",
        "for item in list_items:\n",
        "\n",
        "  print(\"Item: \"+str(item))\n",
        "  y = data[item]\n",
        "  df_forecasts = pd.DataFrame()\n",
        "  df_forecasts['y'] = y\n",
        "\n",
        "  y_timeseries = TimeSeries.from_series(y)\n",
        "  train, test = y_timeseries.split_before(start)\n",
        "\n",
        "  list_errors = []\n",
        "\n",
        "  # for each item, perform the forecast using one model in each loop, and calculate the forecasting errors\n",
        "  # the results are then added to the DataFrame 'df_forecasts'\n",
        "  for model in list_models:\n",
        "    start_time = time.time()\n",
        "    forecast_series = eval_model_rolling_forecasts(model, y, start, fh, stride, series_name = item)\n",
        "    runtime = time.time()-start_time\n",
        "    forecast_series = forecast_series.clip(lower=0)\n",
        "    errors = calculate_test_errors(test.pd_series(), forecast_series, series_name = item, skip_mape = True)\n",
        "    errors['runtime'] = [runtime]\n",
        "    list_errors.append(errors)\n",
        "    df_forecasts[forecast_series.name] = forecast_series\n",
        "\n",
        "  pd_errors = add_ranks(pd.concat(list_errors))\n",
        "\n",
        "  pd_all_errors = pd.concat([pd_all_errors, pd_errors])\n",
        "  list_forecasts.append(df_forecasts)"
      ],
      "metadata": {
        "id": "UyM2SkKHjmue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the errors from all the models for each item\n",
        "pd_all_errors"
      ],
      "metadata": {
        "id": "XZPGECM0pdp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the average errors from all the models across all the items\n",
        "pd_all_errors[pd_all_errors.columns[1:]].groupby('Model').mean().sort_values(by=['Rank_RMSSE'])"
      ],
      "metadata": {
        "id": "LHo7r1HVq4sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list_items[:10]):\n",
        "  plot_forecasts(list_forecasts[i], plot_n_points, series_name = item)"
      ],
      "metadata": {
        "id": "wAHYiLsQrXS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra: Dynamic inventory policy from forecasts\n",
        "\n",
        "The code calculates a dynamic basestock inventory policy using previously generated demand forecasts from various models. In this basestock inventory policy, we assume that the stock must be replenished to the target level ($S_t$) at every replenishment period. This target stock level must ensure that the inventory position (on-hand inventory + in-transit inventory) is sufficient to cover the forecasted demands and safety stock during the supply lead time. More specifically, the dynamic basestock (target) inventory level at time $t$, denoted by $S_t$, is calculated by:\n",
        "\n",
        "$S_t = \\sum_{k=t+1}^{t+L}F_k+SS_t$\n",
        "\n",
        "where $F_t$ is the forecast of the item at time $t$, $L$ is the lead time, and $SS_t$ is the dynamic safety stock at time $t$ calculated as follows:\n",
        "\n",
        "$SS_t = z\\times RMSE \\times \\sqrt{L}$\n",
        "\n",
        "where $RMSE$ is the standard deviation of the forecasting errors (i.e., root mean squared error). This value is calculated from the reference (initial) periods prior to rolling forecasts of that iteration defined by the parameter `n_periods_fcst_errors`. The evaluation of the inventory policy starts from the period following the end of the initial periods.\n",
        "\n",
        "Finally, the code visualizes both the original demand forecasts and the calculated target stock levels against the ideal target stock levels based on the actual demands."
      ],
      "metadata": {
        "id": "QKG5ysDF-ALn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dynamic basestock policy simulation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "item_index = 2\n",
        "\n",
        "forecasts = list_forecasts[item_index][-test_n_points:]\n",
        "forecast_models = forecasts.columns[1:]\n",
        "\n",
        "n_periods_fcst_errors = 13\n",
        "\n",
        "leadtime = 3\n",
        "z_score = 1.96\n",
        "\n",
        "test_forecasts = forecasts[n_periods_fcst_errors:]\n",
        "\n",
        "# prepare the dataframe for the target stock values for each time period that will be calculated\n",
        "target_stocks = pd.DataFrame(index = test_forecasts.index, columns = test_forecasts.columns)\n",
        "\n",
        "hist_rmse = []\n",
        "\n",
        "list_errors = []\n",
        "\n",
        "# calculate the (dynamic) target stock levels for each period in the test periods\n",
        "for starting_period in range(len(test_forecasts.index)-leadtime+1):\n",
        "  initial_forecasts = forecasts[starting_period:starting_period+n_periods_fcst_errors]\n",
        "\n",
        "  actual_leadtime_demand = sum(test_forecasts['y'].iloc[starting_period: starting_period+leadtime])\n",
        "  target_stocks['y'].iloc[starting_period] = actual_leadtime_demand\n",
        "\n",
        "  for model in forecast_models:\n",
        "    squared_diff = (initial_forecasts['y'] - initial_forecasts[model])**2\n",
        "    rmse_val = (np.mean(squared_diff))**(1/2)\n",
        "\n",
        "    exp_leadtime_demand = sum(test_forecasts[model].iloc[starting_period: starting_period+leadtime])\n",
        "    safety_stock = z_score*rmse_val*(leadtime)**(1/2)\n",
        "    target_stocks[model].iloc[starting_period] = exp_leadtime_demand + safety_stock\n",
        "    # print(\"Model[\"+str(model)+\"], SS: \"+str(safety_stock))\n",
        "\n",
        "plot_forecasts(list_forecasts[item_index], plot_n_points)\n",
        "plot_forecasts(target_stocks, plot_n_points, plot_name = 'Target stocks')\n"
      ],
      "metadata": {
        "id": "CbtGAjkMiDgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve and display the forecasting performances for this item based on different models\n",
        "item = list_items[item_index]\n",
        "pd_all_errors[pd_all_errors['SeriesName'] == item]"
      ],
      "metadata": {
        "id": "qzewEHBbW1oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the files needed for the simulation (will be performed in a separate notebook)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "test_forecasts.to_csv('/content/drive/My Drive/test_forecasts.csv')\n",
        "target_stocks.to_csv('/content/drive/My Drive/target_stocks.csv')"
      ],
      "metadata": {
        "id": "o-PBLTI3ZGEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}