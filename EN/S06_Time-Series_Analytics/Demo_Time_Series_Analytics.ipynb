{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg9k+I2zfoQejN6RDTD4/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acedesci/scanalytics/blob/master/EN/S06_Time-Series_Analytics/Demo_Time_Series_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo: Time-series demand forecasting pipeline"
      ],
      "metadata": {
        "id": "XOVvL--ZvrAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Time series demand forecasting is crucial for effective supply chain management. With accurate demand forecasts, businesses can optimize inventory, improve demand planning, enhance sales forecasting, and mitigate supply chain risks.\n",
        "\n",
        "With recent development in Python and opensources, there are many simple-to-use packages such as [statsmodels](https://www.statsmodels.org/stable/index.html) (mainly for statistical techniques), [Prophet (by Facebook)](https://facebook.github.io/prophet/), [GluonTS (by Amazon)] (https://ts.gluon.ai/stable/), and many other libraries. Most libraries require a specific (but similar) data input format and processes. Thus, the most important step to use such packages is to prepare the data in the right format.\n",
        "\n",
        "There are also opensource libaries that are built upon many time-series forecasting packages and provide interfaces to many time-series algorithms such as [sktime](https://www.sktime.net/en/latest/index.html) and [darts](https://unit8co.github.io/darts/). These time-series forecasting interfaces greatly simplify forecasting with its intuitive API and diverse range of models, including classical statistical methods and modern machine learning approaches including deep-learning-based models. An illustration of a time-series pipeline based on the case [1] is depicted below. Note that, in the case, both time-series-based (which requires time-series data format) and machine-learning-based (which requires tabular data format) models are used simultaneously whereas this notebook focuses on the use of time-series models using time-series data format.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://github.com/acedesci/scanalytics/blob/master/EN/S07_Time-Series_Analytics/BBD_forecasts.png?raw=true'/>\n",
        "<figcaption>Forecasting pipeline as depicted in [1]</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "This notebook provide a simple walkthrough for an implementation of time-series forecasting pipeline using `darts`. At the end of the notebook, it also demonstrates time-phased dynamic inventory policy that can be calculated from the forecasts from various forecasting models. The quality of this inventory policy can then be measured through a simulation which has been covered in an earlier session on `DataFrame`.\n",
        "\n",
        "[1] Dodin, P., Xiao, J., Adulyasak, Y., Alamdari, N.E., Gauthier, L., Grangier, P., Lemaitre, P. and Hamilton, W.L., 2023. **Bombardier aftermarket demand forecast with machine learning.** *INFORMS Journal on Applied Analytics*, 53(6), pp.425-445."
      ],
      "metadata": {
        "id": "5GapjXnmwCxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code block description**: We first install `darts` (this must be run every time a new colab notebook is open)."
      ],
      "metadata": {
        "id": "22vwdwz52vRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install darts"
      ],
      "metadata": {
        "id": "K1XzY6lC84P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data inputs\n",
        "\n",
        "This code block primarily  sets up the environment for time series analysis using models available in `darts`.\n",
        "\n",
        "*Notable remarks:*\n",
        "\n",
        "*   `data = pd.read_csv(..., index_col='ds')`: Reads time series data from a CSV file located at the given URL. It assumes the file has a column named 'ds' that represents the dates or timestamps and sets it as the index of the DataFrame.\n",
        "\n",
        "*   `y = data['y']`: Extracts the time series data (likely the demand values) from the 'y' column of the DataFrame and assigns it to the variable y.\n",
        "\n",
        "*   `plot_n_points = 104 `: Sets the number of data points to be used for plotting.\n",
        "\n",
        "*   `test_n_points = 65`: Sets the number of data points to be used for testing the forecasting models."
      ],
      "metadata": {
        "id": "2uSieO9c3AHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import darts\n",
        "from darts.models import AutoARIMA, Prophet, ExponentialSmoothing, Theta, NaiveSeasonal, Croston, NBEATSModel\n",
        "from darts.models import NaiveEnsembleModel, RegressionEnsembleModel\n",
        "from darts.utils.utils import ModelMode, SeasonalityMode\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "from darts.metrics import mape, smape, mase, rmsse, rmse, merr\n",
        "from darts import TimeSeries\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# read historical time-series data (demand)\n",
        "data = pd.read_csv('https://bit.ly/m5simple', index_col='ds')\n",
        "data.index = pd.to_datetime(data.index)\n",
        "n_periods = len(data.index)\n",
        "\n",
        "# obtain the series of time series from column 'y'\n",
        "y = data['y']\n",
        "\n",
        "# number of historical data points to show on the plots (in weeks)\n",
        "plot_n_points = 104\n",
        "\n",
        "# number of data points used as the test set\n",
        "test_n_points = 65\n"
      ],
      "metadata": {
        "id": "tyAN4usQAVfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define key functions\n",
        "\n",
        "This code block defines the following functions that will be called often in the pipeline.\n",
        "\n",
        "**Function** `calculate_test_errors`:\n",
        "\n",
        "This function calculates and return various error metrics for a time series forecast.\n",
        "\n",
        "Input:\n",
        "\n",
        "`test_series`: The actual values of the time series during the testing period.\n",
        "`forecast_series`: The predicted values generated by a forecasting model during the testing period.\n",
        "\n",
        "Process:\n",
        "\n",
        "It uses functions from the darts.metrics module (`mape, smape, rmsse, rmse, merr`) to calculate the following error metrics (see [Darts error metrics](https://unit8co.github.io/darts/generated_api/darts.metrics.html)):\n",
        "\n",
        "\n",
        "* MAPE: Mean Absolute Percentage Error (from function `mape`)\n",
        "* sMAPE: Symmetric Mean Absolute Percentage Error (from function `smape`)\n",
        "* RMSSE: Root Mean Squared Scaled Error (from function `rmsse`)\n",
        "* RMSE: Root Mean Squared Error (from function `rmse`)\n",
        "* ME: Mean Error (from function `merr`)\n",
        "* Norm_ME: Mean Error normalized by the mean of actual demand\n",
        "\n",
        "Output:\n",
        "The function returns the errors DataFrame containing the calculated error metrics for the model.\n",
        "\n",
        "**Function** `add_ranks`:\n",
        "\n",
        "This function is used to add rank columns to a DataFrame based on the values in existing columns, It returns the modified DataFrame with the added rank columns."
      ],
      "metadata": {
        "id": "dQtUdpyeOgk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# functions to return test errors\n",
        "\n",
        "def calculate_test_errors(test_series, forecast_series, series_name = 'y', print_results = False, skip_mape = False):\n",
        "  m_name = forecast_series.name\n",
        "  test = TimeSeries.from_series(test_series)\n",
        "  forecast = TimeSeries.from_series(forecast_series)\n",
        "\n",
        "  # we need to skip MAPE if we have a zero demand observation since we cannot divide the error by zero\n",
        "  # this is currently done manually. In any case, darts will also return an error if there is a zero demand when calculating mape.\n",
        "  if not skip_mape:\n",
        "    m_mape = mape(test, forecast)\n",
        "    m_smape = smape(test, forecast)\n",
        "\n",
        "  m_rmsse = rmsse(test, forecast, insample = train)\n",
        "  m_rmse = rmse(test, forecast)\n",
        "  m_merr = merr(test, forecast)\n",
        "\n",
        "  if print_results:\n",
        "    if not skip_mape:\n",
        "      print(f\"model {m_name} obtains Mean Absolute Percentage Error: {m_mape:.2f}%\")\n",
        "      print(f\"model {m_name} obtains symmetric Mean Absolute Percentage Error: {m_smape:.2f}%\")\n",
        "\n",
        "    print(f\"model {m_name} obtains Root Mean Squared Scaled Error: {m_rmsse:.2f}\")\n",
        "    print(f\"model {m_name} obtains Root Mean Squared Error: {m_rmse:.2f}\")\n",
        "    print(f\"model {m_name} obtains Mean Error: {m_merr:.2f}\")\n",
        "\n",
        "  test_mean = test.pd_series().mean()\n",
        "\n",
        "  if not skip_mape:\n",
        "    column_names=['SeriesName', 'Model', 'MAPE', 'sMAPE', 'RMSSE', 'RMSE', 'Norm_RMSE', 'ME', 'Norm_ME']\n",
        "    errors = pd.DataFrame(columns=column_names)\n",
        "    errors.loc[0] = [series_name, m_name, m_mape, m_smape, m_rmsse, m_rmse, m_rmse/test_mean, m_merr, m_merr/test_mean]\n",
        "  else:\n",
        "    column_names=['SeriesName', 'Model', 'RMSSE', 'RMSE', 'Norm_RMSE', 'ME', 'Norm_ME']\n",
        "    errors = pd.DataFrame(columns=column_names)\n",
        "    errors.loc[0] = [series_name, m_name, m_rmsse, m_rmse, m_rmse/test_mean, m_merr, m_merr/test_mean]\n",
        "\n",
        "  return errors\n",
        "\n",
        "def add_ranks(df, ascending=True):\n",
        "  for col in df.columns[2:]:\n",
        "    df['Rank_'+col] = df[col].abs().rank(ascending=ascending)\n",
        "  return df\n",
        "\n"
      ],
      "metadata": {
        "id": "TP9O_RLsLFyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline 1: Forecasting and evaluation using two-fold validation\n",
        "\n",
        "The code block calls and evaluates various time series forecasting models in the `darts` library. It first prepares the data, visualizes the actual values, and defines a list of forecasting models, including ensembles. Then, it iteratively trains each model, generates forecasts, calculates error metrics. Finally, it combines and displays the errors and ranks of each model, providing a performance comparison based on different models for the given dataset.\n",
        "\n",
        "The training and evaluation of this pipeline is based on a two-fold validation process where data is split only into one training and one test set at a given split point. The predictions are evaluated against the actual values in the test set (which is not used in the training process).\n"
      ],
      "metadata": {
        "id": "nfVzOJqp5cHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single split (two-fold)\n",
        "\n",
        "plt.figure(figsize=(18,6))\n",
        "\n",
        "sns.scatterplot(x = y[-plot_n_points:].index, y = y[-plot_n_points:].values, label = 'true')\n",
        "\n",
        "y_timeseries = TimeSeries.from_series(y)\n",
        "\n",
        "ensemble_models = [NaiveSeasonal(K=52), Theta(season_mode = SeasonalityMode.ADDITIVE), Croston()]\n",
        "\n",
        "list_models = [NaiveSeasonal(K=52),\n",
        "               ExponentialSmoothing(seasonal = None),\n",
        "               AutoARIMA(), \\\n",
        "               Theta(), \\\n",
        "               # Prophet(), \\\n",
        "               # NBEATSModel(input_chunk_length=52, output_chunk_length=52, n_epochs=50), \\\n",
        "               Croston(),\\\n",
        "               Croston(version='sba'),\\\n",
        "               NaiveEnsembleModel(forecasting_models=ensemble_models), \\\n",
        "               RegressionEnsembleModel(forecasting_models=ensemble_models, regression_train_n_points=13)]\n",
        "\n",
        "list_errors = []\n",
        "df_forecasts = pd.DataFrame()\n",
        "df_forecasts['y'] = y\n",
        "\n",
        "start = len(y)-test_n_points\n",
        "train, test = y_timeseries.split_before(start)\n",
        "\n",
        "for model in list_models:\n",
        "  m_name = str(model)[:15]\n",
        "  model.fit(train)\n",
        "  forecast_series = model.predict(len(test)).pd_series()\n",
        "  forecast_series.name = m_name\n",
        "  errors = calculate_test_errors(test.pd_series(), forecast_series, series_name = 'y')\n",
        "  sns.lineplot(data = forecast_series, label = forecast_series.name)\n",
        "  list_errors.append(errors)\n",
        "  df_forecasts[forecast_series.name] = forecast_series\n",
        "\n",
        "pd_errors = pd.concat(list_errors, ignore_index = True)\n",
        "pd_errors = add_ranks(pd_errors)\n",
        "\n",
        "pd_errors"
      ],
      "metadata": {
        "id": "tvFLbMGD7m9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define rolling forecasts and plot functions\n",
        "\n",
        "**Function** `eval_model_rolling_forecasts`:\n",
        "\n",
        "This function simulates a real-world forecasting scenario where one would retrain the model periodically with new data to make future predictions. It provides a more realistic evaluation of the model's performance over time compared to a single train-test split. This functions requires the **following inputs**:\n",
        "\n",
        "* `model`: darts models (see [darts forecasting models](https://unit8co.github.io/darts/generated_api/darts.models.forecasting.html))\n",
        "* `series`: a series containing the entire time-series data\n",
        "* `start`: the first starting point of the forecasting evaluation\n",
        "* `fh`: length of the forecast horizon\n",
        "* `stride`: the retraining (forecast update) internal (i.e., new forecasts are produced every `stride` periods). This value must be $\\leq$ `fh`\n",
        "\n",
        "The model returns a series containing the predictions that are produced using a rolling update of every `stride` periods. Only the most recent values of the predictions are kept in the series.\n",
        "\n",
        "**Function** `plot_forecasts`:\n",
        "\n",
        " This function is used to visualize time series data from a DataFrame containing actual and forecasted data.It generates a plot with actual values displayed as scatter points and forecasts from different models represented as lines."
      ],
      "metadata": {
        "id": "z-_si9uFQVCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def eval_model_rolling_forecasts(model, series, start, fh, stride, series_name = 'y'):\n",
        "  m_name = str(model)[:15]\n",
        "\n",
        "  timeseries = TimeSeries.from_series(series)\n",
        "  list_hist_rolling_forecasts = []\n",
        "  for starting_point in range(start, len(timeseries), stride):\n",
        "    train, val = timeseries.split_before(starting_point)\n",
        "    forecast_horizon = min(fh, len(val))\n",
        "    forecasts = model.fit(train).predict(forecast_horizon)\n",
        "    list_hist_rolling_forecasts.append(forecasts.pd_series())\n",
        "\n",
        "  last_fold = len(list_hist_rolling_forecasts)-1\n",
        "  hist_rolling_forecasts = list_hist_rolling_forecasts[last_fold]\n",
        "\n",
        "  for fold in range(last_fold):\n",
        "    hist_rolling_forecasts = pd.concat([hist_rolling_forecasts, list_hist_rolling_forecasts[fold][:stride]])\n",
        "\n",
        "  hist_rolling_forecasts.name = m_name\n",
        "  print(\"\\t----- generating rolling forecasts using model \" +str(hist_rolling_forecasts.name))\n",
        "\n",
        "  return hist_rolling_forecasts\n"
      ],
      "metadata": {
        "id": "LzcpyAeqYWVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_forecasts(df_forecasts, n_points, series_name = 'y', plot_name = 'Prediction'):\n",
        "  plt.figure(figsize=(18,6))\n",
        "  plt.title(plot_name+\": \" + series_name)\n",
        "  plt.xlabel(\"Date\")\n",
        "  plt.ylabel(\"Value\")\n",
        "  y = df_forecasts['y']\n",
        "  sns.scatterplot(x = y[-n_points:].index, y = y[-n_points:].values, label = 'true')\n",
        "\n",
        "  for model in df_forecasts.columns[1:]:\n",
        "    sns.lineplot(data = df_forecasts[model][-n_points:], label = model)\n"
      ],
      "metadata": {
        "id": "73rxmhKR8yJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline 2: Forecasting and evaluation using sequential validation\n",
        "\n",
        "This code block is similar to pipeline 1 but it simulates and evaluates the forecasts on a rolling basis using the previously defined function `eval_model_rolling_forecasts(...)`."
      ],
      "metadata": {
        "id": "KfvRhnGx6zBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rolling forecasts evaluation (multifold sequential validation)\n",
        "\n",
        "val_size = test_n_points\n",
        "y_timeseries = TimeSeries.from_series(y)\n",
        "start = len(y)-val_size\n",
        "fh = 13\n",
        "stride=13\n",
        "\n",
        "ensemble_models = [NaiveSeasonal(K=52), Theta(season_mode = SeasonalityMode.ADDITIVE), Croston()]\n",
        "\n",
        "list_models = [NaiveSeasonal(K=52),\n",
        "               ExponentialSmoothing(),\n",
        "               AutoARIMA(), \\\n",
        "               Theta(), \\\n",
        "               # Prophet(), \\\n",
        "               # NBEATSModel(input_chunk_length=52, output_chunk_length=52, n_epochs=50), \\\n",
        "               Croston(),\\\n",
        "               NaiveEnsembleModel(forecasting_models=ensemble_models), \\\n",
        "               RegressionEnsembleModel(forecasting_models=ensemble_models, regression_train_n_points=13)\n",
        "               ]\n",
        "\n",
        "list_errors = []\n",
        "df_forecasts = pd.DataFrame()\n",
        "df_forecasts['y'] = y\n",
        "train, test = y_timeseries.split_before(start)\n",
        "\n",
        "for model in list_models:\n",
        "  forecast_series = eval_model_rolling_forecasts(model, y, start, fh, stride)\n",
        "  errors = calculate_test_errors(test.pd_series(), forecast_series)\n",
        "  list_errors.append(errors)\n",
        "  df_forecasts[forecast_series.name] = forecast_series\n",
        "\n",
        "pd_errors = pd.concat(list_errors)\n",
        "pd_errors = add_ranks(pd_errors)\n",
        "\n",
        "plot_forecasts(df_forecasts, plot_n_points)\n",
        "\n",
        "pd_errors"
      ],
      "metadata": {
        "id": "SbgfU-0t7lFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline 3: Automating forecasting and sequential validation on multiple items\n",
        "\n",
        "This code block is similar to pipeline 2 that performs rolling forecasts. The code automate the predictions of multiple items through iterative process."
      ],
      "metadata": {
        "id": "Q_pe_lE99cq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline to run forecast for multiple items\n",
        "\n",
        "import time\n",
        "\n",
        "# state-item level\n",
        "url = 'https://raw.githubusercontent.com/acedesci/scanalytics/refs/heads/master/EN/data/M5/processed_data/m5data_CA_item_state_rand.csv'\n",
        "\n",
        "# store-item level (slow-moving)\n",
        "# url = 'https://raw.githubusercontent.com/acedesci/scanalytics/refs/heads/master/EN/data/M5/processed_data/m5data_CA_item_store_n25_cv.csv'\n",
        "\n",
        "data = pd.read_csv(url, index_col='ds')\n",
        "data.index = pd.to_datetime(data.index)\n",
        "\n",
        "list_items = data.columns[:3]\n",
        "print(\"list of items: \" + str(list_items))\n",
        "\n",
        "val_size = test_n_points\n",
        "start = len(data.index)-val_size\n",
        "fh = 13\n",
        "stride = 13\n",
        "\n",
        "ensemble_models = [NaiveSeasonal(K=52), Theta(season_mode = SeasonalityMode.ADDITIVE), Croston()]\n",
        "\n",
        "list_models = [NaiveSeasonal(K=52),\n",
        "               # ExponentialSmoothing(),\n",
        "               AutoARIMA(), \\\n",
        "               Theta(season_mode = SeasonalityMode.ADDITIVE), \\\n",
        "               # Prophet(), \\\n",
        "               # NBEATSModel(input_chunk_length=52, output_chunk_length=52, n_epochs=50), \\\n",
        "               Croston(),\\\n",
        "               NaiveEnsembleModel(forecasting_models=ensemble_models), \\\n",
        "               RegressionEnsembleModel(forecasting_models=ensemble_models, regression_train_n_points=13)\n",
        "               ]\n",
        "\n",
        "pd_all_errors = pd.DataFrame()\n",
        "\n",
        "list_forecasts = []\n",
        "\n",
        "for item in list_items:\n",
        "\n",
        "  print(\"Item: \"+str(item))\n",
        "  y = data[item]\n",
        "  df_forecasts = pd.DataFrame()\n",
        "  df_forecasts['y'] = y\n",
        "\n",
        "  y_timeseries = TimeSeries.from_series(y)\n",
        "  train, test = y_timeseries.split_before(start)\n",
        "\n",
        "  list_errors = []\n",
        "\n",
        "  for model in list_models:\n",
        "    start_time = time.time()\n",
        "    forecast_series = eval_model_rolling_forecasts(model, y, start, fh, stride, series_name = item)\n",
        "    runtime = time.time()-start_time\n",
        "    forecast_series = forecast_series.clip(lower=0)\n",
        "    errors = calculate_test_errors(test.pd_series(), forecast_series, series_name = item, skip_mape = True)\n",
        "    errors['runtime'] = [runtime]\n",
        "    list_errors.append(errors)\n",
        "    df_forecasts[forecast_series.name] = forecast_series\n",
        "\n",
        "  pd_errors = add_ranks(pd.concat(list_errors))\n",
        "\n",
        "  pd_all_errors = pd.concat([pd_all_errors, pd_errors])\n",
        "  list_forecasts.append(df_forecasts)"
      ],
      "metadata": {
        "id": "UyM2SkKHjmue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the errors from all the models for each item\n",
        "pd_all_errors"
      ],
      "metadata": {
        "id": "XZPGECM0pdp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the average errors from all the models across all the items\n",
        "pd_all_errors[pd_all_errors.columns[1:]].groupby('Model').mean().sort_values(by=['Rank_RMSSE'])"
      ],
      "metadata": {
        "id": "LHo7r1HVq4sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list_items[:10]):\n",
        "  plot_forecasts(list_forecasts[i], plot_n_points, series_name = item)"
      ],
      "metadata": {
        "id": "wAHYiLsQrXS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra: Dynamic inventory policy from forecasts\n",
        "\n",
        "The code calculates a dynamic basestock inventory policy using previously generated demand forecasts from various models. More specifically, the dynamic basestock (target) inventory level at time $t$, denoted by $S_t$, is calculated by:\n",
        "\n",
        "$S_t = \\sum_{k=t+1}^{t+L}F_k+SS_t$\n",
        "\n",
        "where $L$ is the lead time, and $SS_t$ is the dynamic safety stock calculated as follows:\n",
        "\n",
        "$SS_t = z\\times RMSE \\times \\sqrt{L}$\n",
        "\n",
        "where $RMSE$ is the standard deviation of the forecasting errors (i.e., root mean squared error). This value is calculated from the reference (initial) periods prior to rolling forecasts of that iteration defined by the parameter `n_periods_fcst_errors`. The evaluation of the inventory policy starts from the period following the end of the initial periods.\n",
        "\n",
        "Finally, the code visualizes both the original demand forecasts and the calculated target stock levels against the ideal target stock levels based on the actual demands."
      ],
      "metadata": {
        "id": "QKG5ysDF-ALn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dynamic basestock policy simulation\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "item_index = 2\n",
        "\n",
        "forecasts = list_forecasts[item_index][-test_n_points:]\n",
        "forecast_models = forecasts.columns[1:]\n",
        "\n",
        "n_periods_fcst_errors = 13\n",
        "\n",
        "leadtime = 3\n",
        "z_score = 1.96\n",
        "\n",
        "test_forecasts = forecasts[n_periods_fcst_errors:]\n",
        "target_stocks = pd.DataFrame(index = test_forecasts.index, columns = test_forecasts.columns)\n",
        "\n",
        "hist_rmse = []\n",
        "\n",
        "list_errors = []\n",
        "\n",
        "for starting_period in range(len(test_forecasts.index)-leadtime+1):\n",
        "  initial_forecasts = forecasts[starting_period:starting_period+n_periods_fcst_errors]\n",
        "\n",
        "  actual_leadtime_demand = sum(test_forecasts['y'].iloc[starting_period: starting_period+leadtime])\n",
        "  target_stocks['y'].iloc[starting_period] = actual_leadtime_demand\n",
        "\n",
        "  for model in forecast_models:\n",
        "    squared_diff = (initial_forecasts['y'] - initial_forecasts[model])**2\n",
        "    rmse_val = (np.mean(squared_diff))**(1/2)\n",
        "\n",
        "    exp_leadtime_demand = sum(test_forecasts[model].iloc[starting_period: starting_period+leadtime])\n",
        "    safety_stock = z_score*rmse_val*(leadtime)**(1/2)\n",
        "    target_stocks[model].iloc[starting_period] = exp_leadtime_demand + safety_stock\n",
        "    # print(\"Model[\"+str(model)+\"], SS: \"+str(safety_stock))\n",
        "\n",
        "plot_forecasts(list_forecasts[item_index], plot_n_points)\n",
        "plot_forecasts(target_stocks, plot_n_points, plot_name = 'Target stocks')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CbtGAjkMiDgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve and display the forecasting performances for this item based on different models\n",
        "item = list_items[item_index]\n",
        "pd_all_errors[pd_all_errors['SeriesName'] == item]"
      ],
      "metadata": {
        "id": "qzewEHBbW1oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the files needed for the simulation (will be performed in a separate notebook)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "test_forecasts.to_csv('/content/drive/My Drive/test_forecasts.csv')\n",
        "target_stocks.to_csv('/content/drive/My Drive/target_stocks.csv')"
      ],
      "metadata": {
        "id": "o-PBLTI3ZGEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}