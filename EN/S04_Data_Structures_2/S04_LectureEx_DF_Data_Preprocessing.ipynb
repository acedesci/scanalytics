{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acedesci/scanalytics/blob/master/EN/S04_Data_Structures_2/S04_LectureEx_DF_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpcq0W5adVgo"
      },
      "source": [
        "# S4 - Data Preprocessing and SCM examples using DataFrame\n",
        "Programming topics covered in this section:\n",
        "* Data preprocessing\n",
        "\n",
        "Examples include:\n",
        "* Exploring Supply Chain health commodity shipment and pricing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWUOy93mdlkW"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o-4MMzmdVg0"
      },
      "source": [
        "## 1. Importing data and creating a report\n",
        "In this exercise, we will explore some adapted data set which provides supply chain health commodity shipment and pricing data. Specifically, the data set identifies Antiretroviral (ARV) and HIV lab shipments to supported countries. In addition, the data set provides the commodity pricing and associated supply chain expenses necessary to move the commodities to countries for use. The original data are provided by the US Agency for International Development and can be accessed at [this page](https://catalog.data.gov/dataset/supply-chain-shipment-pricing-data).\n",
        "\n",
        "This is a description of our adapted data in the file `SCMS_Delivery_History_Dataset.csv`.\n",
        "\n",
        "| VARIABLE NAME | DESCRIPTION |\n",
        "|:----|:----|\n",
        "|id| identification number|\n",
        "|project code|identification of the project|\n",
        "|country|country to which the items are shipped|\n",
        "|vendor|identification of the vendor of the item|\n",
        "|manufacturing site|name of the manufacturer of the item|\n",
        "|shipment mode|transportation mode (e.g., air, truck)|\n",
        "|scheduled delivery date|programmed date for delivery|\n",
        "|delivered to client date|real date of delivery|\n",
        "|delivery recorded date|registered date of delivery|\n",
        "|product group|main category of the item|\n",
        "|product subgroup|subcategory of the item (e.g., HIV test, pediatric, Adult) |\n",
        "|molecule type|description of the composition of the item (e.g., Nevirapine, HIV 1/2, Didanosine)|\n",
        "|brand| item brand (e.g, generic or any other commercial brand)|\n",
        "|dosage| specifications about the dosage of each item (e.g.,10mg/ml, 200mg)|\n",
        "|dosage form|instructions for consumption (e.g., capsule, tablet, oral solution) |\n",
        "|units per pack| number of units in each package|\n",
        "|quantity pack sold| number of packages shipped to the specified country|\n",
        "|value sold| total value in $\\$$ USD of the shipment (i.e., pack_price * quantity pack sold|\n",
        "|pack price| price in $\\$$ USD per package|\n",
        "|unit price| price in $\\$$ USD per unit|\n",
        "|weight (kilograms)| total weight in kilograms of the shipment|\n",
        "|freight cost (usd)| value in $\\$$USD paid for transportation|\n",
        "|insurance (usd)|value in $\\$$USD paid for insurance|\n",
        "\n",
        "\n",
        "\n",
        "Let's import our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8cvKdP0cdVg0"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/acedesci/scanalytics/master/EN/S04_Data_Structures_2/SC_Shipment_Pricing_Data.csv'\n",
        "df_SC = pd.read_csv(url)  # reading data file into a DataFrame\n",
        "\n",
        "df_SC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_SC.columns"
      ],
      "metadata": {
        "id": "jCLEGxnwEvU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nyoij1WqGs3"
      },
      "source": [
        "As the table is relatively large, we only choose our column of interest to be put in the DataFrame:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDYrEy_9qWhz"
      },
      "source": [
        "selected_columns = ['id', 'country', 'shipment_mode', 'scheduled_delivery_date',\n",
        "       'item_id', 'item_description', 'brand', 'line_item_quantity',\n",
        "       'line_item_value', 'unit_price', 'weight_kilograms', 'freight_cost_usd',\n",
        "       'line_item_insurance_usd']\n",
        "\n",
        "df_SC = df_SC[selected_columns].copy() # we make a copy of this to avoid any issue\n",
        "df_SC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4HuG2H3dVjV"
      },
      "source": [
        "---\n",
        "## 2. Preprocessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y7Cp120dvtH"
      },
      "source": [
        "We can use `df.describe()` function to show descriptive statistics of the numerical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgYsLH2VdVjW"
      },
      "source": [
        "df_SC.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUZ7rOsqdVjX"
      },
      "source": [
        "Let's take a look at the type of data in our `DataFrame`. We can notice that column `scheduled_delivery_date` is `object`, which means it can be string or mixed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "m5YmsjiBdVjX"
      },
      "source": [
        "df_SC.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhXSuCSmdVjY"
      },
      "source": [
        "Let's convert the data in column `schedule_delivery_date` to the correct format, as presented below. We can obtain the same results using the function `DataFrame.astype('datatime64')`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "VBq-cx44dVjY"
      },
      "source": [
        "# here we replace the original column with the newly formatted one\n",
        "df_SC['scheduled_delivery_date'] = pd.to_datetime(df_SC['scheduled_delivery_date'])\n",
        "df_SC['scheduled_delivery_date']\n",
        "df_SC.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QDegoL8dVja"
      },
      "source": [
        "We can also see that the columns `weight_kilograms` and `freight_cost_usd` are also of type `object`. These data should be a numeric value since it represents kilograms and $USD. However, the raw data have some annotations made by the user, as you can see below by printing the first 10 rows of your DataFrame, so this is why it is recognized as type `object`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXRFOs-vdVjb"
      },
      "source": [
        "df_SC[['weight_kilograms','freight_cost_usd']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqLOQ64sdVjc"
      },
      "source": [
        "We can then use the `to_numeric` method in order to convert the values under the `weight_kilograms` and `freight_cost_usd`  columns into a float:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vl3msxLdVjc"
      },
      "source": [
        "df_SC['weight_kilograms'] = pd.to_numeric(df_SC['weight_kilograms'], errors='coerce')\n",
        "df_SC['freight_cost_usd'] = pd.to_numeric(df_SC['freight_cost_usd'], errors='coerce')\n",
        "df_SC[['weight_kilograms','freight_cost_usd']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUwIhxugdVjd"
      },
      "source": [
        "By setting `errors='coerce'`, you will transform the non-numeric values into `NaN`.\n",
        "Now we can obtain some descriptive statistics for `weight_kg` and `freight_cost_usd` using the `describe()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "A0a2bja6dVjd"
      },
      "source": [
        "df_SC.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftTbWTsDdVje"
      },
      "source": [
        "---\n",
        "## 3. Missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB_dKErgdVje"
      },
      "source": [
        "Now let's take a look at the missing values in our DataFrame. We can see how many missing values we have at each column as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fuSJ-rVvdVjf"
      },
      "source": [
        "df_SC.isnull().sum()\n",
        "df_SC\n",
        "df_SC[df_SC['shipment_mode'].isna()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTi_pKl-dVjf"
      },
      "source": [
        "We can notice that there are some missing values in the column `shipment_mode`, which represents the transportation mode (e.g., by air). There is not that much we can do in order to replace these missing values with meaningful information, so we'll replace the missing values of in these columns with the word `'missing'`. We use the `.fillna()` method with the option `inplace=True` to save the changes in our original DataFrame. Check [this page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) for more information about the `.fillna()` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "b5VYG7_4dVjg"
      },
      "source": [
        "df_SC['shipment_mode'].fillna(value='missing', inplace=True)\n",
        "print(df_SC.isnull().sum())\n",
        "df_SC[df_SC['shipment_mode'] == 'missing']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcHFS47z3GeC"
      },
      "source": [
        "df_SC.line_item_insurance_usd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASqs4HeSdVjg"
      },
      "source": [
        "Now we will replace the missing values in column `line_item_insurance_usd` by an approximated value, computed as the mean value for this column. Note that we create a new column for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUXOZFzCdVjg"
      },
      "source": [
        "# calculate the mean of the entire column\n",
        "insurance_mean = df_SC['line_item_insurance_usd'].mean()\n",
        "\n",
        "# fill in the missing value (NaN) using the mean of the entire column\n",
        "df_SC['item_insurance_rp_mean'] = df_SC['line_item_insurance_usd'].fillna(insurance_mean)\n",
        "\n",
        "# show the new columns in the updated dataframe\n",
        "df_SC[['line_item_insurance_usd','item_insurance_rp_mean']][df_SC['line_item_insurance_usd'].isna() == True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the global mean may not reflect the insurance of each country, we can instead replace the missing values in column line_item_insurance_usd by the mean value of the insurance of the corresponding country. Here we can use an iterative function to go through each row to fill in the missing value in the newly created column."
      ],
      "metadata": {
        "id": "clzhKX1Q4rvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first, copy the original column to the new column\n",
        "df_SC['item_insurance_rp_groupmean'] =  df_SC['line_item_insurance_usd']\n",
        "\n",
        "# use the groupby function to obtain the mean value for each group\n",
        "insurance_groupmean = df_SC.groupby('country')['line_item_insurance_usd'].mean()\n",
        "\n",
        "# go through each row and fill in missing data using the group mean\n",
        "for row in df_SC.index:\n",
        "  if pd.isna(df_SC.loc[row, 'item_insurance_rp_groupmean']):\n",
        "\n",
        "    # obtain the country name\n",
        "    country = df_SC.loc[row, 'country']\n",
        "\n",
        "    # set the missing value to the mean of the corresponding country\n",
        "    df_SC.loc[row, 'item_insurance_rp_groupmean'] = insurance_groupmean[country]\n",
        "\n",
        "df_SC[['country','line_item_insurance_usd','item_insurance_rp_mean', 'item_insurance_rp_groupmean']][df_SC['line_item_insurance_usd'].isna() == True]"
      ],
      "metadata": {
        "id": "f442PtbN3iHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The issue about replacing the NaN value by the mean or another constant is that it disregards other features/values of the data point. To overcome this is issue, we can opt to impute each missing value by using an estimation or prediction based on other data points. One practical method is to use the k-nearest neighbors (KNN) algorithm for imputation. More details can be found in this link1 and this link2 This can be done using the code below which take the weighted sum of the values from the two closest data points to each missing value.\n"
      ],
      "metadata": {
        "id": "obdkfE2w-Kug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sklearn\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# First we determine which columns we want to use as references for matching\n",
        "feature_input_columns = ['line_item_value','weight_kilograms', 'freight_cost_usd']\n",
        "# Copy only the relevant columns to the new DataFrame\n",
        "df_SC_kNN = df_SC[feature_input_columns].copy()\n",
        "# Add the column `line_item_insurance_(usd)` which we want to impute and assigns a new name to it\n",
        "df_SC_kNN['item_insurance_rp_kNN'] = df_SC['line_item_insurance_usd']\n",
        "\n",
        "# Set up the imputer object with the number of neighbors = 3\n",
        "imputer = sklearn.impute.KNNImputer(n_neighbors=3)\n",
        "# Train to determine the neighbors and estimate the new values for all the missing values.\n",
        "# This process might be time consuming if the size of the data and neighborhood are large.\n",
        "# The new values are then assigned back to the DataFrame. # Since the method return an array (not a DataFrame),\n",
        "# we need to assign it back to the array values of the DataFrame, i.e., `df_SC_kNN[:]`\n",
        "df_SC_kNN[:] = imputer.fit_transform(df_SC_kNN)\n",
        "\n",
        "# copy the new column to t\n",
        "df_SC['item_insurance_rp_kNN'] = df_SC_kNN['item_insurance_rp_kNN'].copy()\n",
        "# Now we can examine the results\n",
        "df_SC[['item_insurance_rp_mean', 'item_insurance_rp_groupmean','item_insurance_rp_kNN']][df_SC['line_item_insurance_usd'].isna() == True]"
      ],
      "metadata": {
        "id": "QWc1qufT-H0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCojijHIdVjh"
      },
      "source": [
        "---\n",
        "## 4. Data Transformation\n",
        "\n",
        "### Scaling methods\n",
        "Variables tend to have different ranges and some algorithms are adversely affected by differences in variable ranges. Variables with greater ranges tend to have larger influence on data modelâ€™s results. Therefore, numeric field values may need to be standardized/normalized.\n",
        "\n",
        "From the output of the `describe()` method in the previous line of code, we can notice that the numerical variables have different ranges. For instance, `units_per_pack` varies from 1 to 1000, while `weight_kg` varies from 0 to 857354. We would like to apply normalization method to scale the numerical values in our data.\n",
        "\n",
        "Let's apply the **min-max normalization** and **z-score normalization** methods, by identifying how much greater the field value is than the minimum value, and scaling this difference by the range of field values.\n",
        "\n",
        "$$X^{norm}_i=\\frac{X_i-X_{min}}{X_{max}-X_{min}}$$\n",
        "and\n",
        "$$Z_i=\\frac{X_i-X_{mean}}{\\sigma_X}$$\n",
        "Thus,  I compute the normalized version of each of the numerical variable and add this as a new column of our data frame. We can proceed as follows.\n",
        "\n",
        "First, we create a list of the columns we want to normalize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9SeAs_CdVjh"
      },
      "source": [
        "columns_to_norm = ['line_item_value','weight_kilograms', 'freight_cost_usd']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uyUgamUdVji"
      },
      "source": [
        "Then, I can create a `for` loop to compute the normalized version for each one of these columns and add it to `df_SC`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDapVaosdVji"
      },
      "source": [
        "for col in columns_to_norm:\n",
        "    df_SC[col + '_minmax'] = (df_SC[col] - df_SC[col].min())/(df_SC[col].max() - df_SC[col].min())   # add the new normalized column\n",
        "    df_SC[col + '_z'] = (df_SC[col] - df_SC[col].mean())/df_SC[col].std()    # add the new z-norm column\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQm838xWUlZU"
      },
      "source": [
        "df_SC[['line_item_value','line_item_value_minmax','line_item_value_z']].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***A more robust kNN***: KNNImputer uses the Euclidean distance metric by default to find the nearest neighbors. If features have different scales, the feature with the largest range will significantly influence the distance calculation and thus the imputation result. For example, a feature with values from 1 to 10000 will have a much greater impact than one with values from 1 to 10. We can run kNN again using normalized features rather than the original feature values."
      ],
      "metadata": {
        "id": "rQV_CsMZnDBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# First we determine which columns we want to use as references for matching. Now we will use the normalized columns.\n",
        "feature_input_columns = ['line_item_value_z','weight_kilograms_z', 'freight_cost_usd_z']\n",
        "# Copy only the relevant columns to the new DataFrame\n",
        "df_SC_kNN_norm = df_SC[feature_input_columns].copy()\n",
        "# Add the column `line_item_insurance_(usd)` which we want to impute and assigns a new name to it\n",
        "df_SC_kNN_norm['item_insurance_rp_kNN_norm'] = df_SC['line_item_insurance_usd']\n",
        "\n",
        "# Set up the imputer object with the number of neighbors = 3\n",
        "imputer = sklearn.impute.KNNImputer(n_neighbors=3)\n",
        "# Train to determine the neighbors and estimate the new values for all the missing values.\n",
        "# This process might be time consuming if the size of the data and neighborhood are large.\n",
        "# The new values are then assigned back to the DataFrame. # Since the method return an array (not a DataFrame),\n",
        "# we need to assign it back to the array values of the DataFrame, i.e., `df_SC_kNN[:]`\n",
        "df_SC_kNN_norm[:] = imputer.fit_transform(df_SC_kNN_norm)\n",
        "\n",
        "# copy the new column to t\n",
        "df_SC['item_insurance_rp_kNN_norm'] = df_SC_kNN_norm['item_insurance_rp_kNN_norm'].copy()\n",
        "# Now we can examine the results. We can first define the conditions to filter the rows where the imputed values in the two columns are different.\n",
        "filter_condition = (df_SC['line_item_insurance_usd'].isna() == True) & (df_SC['item_insurance_rp_kNN'] != df_SC['item_insurance_rp_kNN_norm'])\n",
        "# Then use this filter to select and display only the rows that correspond to the filter.\n",
        "df_SC[['item_insurance_rp_mean', 'item_insurance_rp_groupmean','item_insurance_rp_kNN','item_insurance_rp_kNN_norm']][filter_condition]"
      ],
      "metadata": {
        "id": "E2C0kHN_nCTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xksxip07bC0w"
      },
      "source": [
        "In order to deal with **skewed data**, we can also use a scaling technique (such as log transformation). For example, we ca transform the column `line_item_value` as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK0ztiClWzOz"
      },
      "source": [
        "# first we evaluate the distribution of the value\n",
        "df_SC[['line_item_value']].plot(kind = 'hist')\n",
        "\n",
        "# apply the transformation\n",
        "import math\n",
        "\n",
        "# sqrt transformation\n",
        "df_SC['line_item_value_sqrt'] = [math.sqrt(x) for x in df_SC['line_item_value']]\n",
        "\n",
        "# note that math.log calculates a natural log (i.e., ln(x)). Using x + 1 is a workaround to deal with 0\n",
        "df_SC['line_item_value_log'] = [math.log(x+1) for x in df_SC['line_item_value']]\n",
        "\n",
        "\n",
        "# plot to compare the results\n",
        "df_SC[['line_item_value','line_item_value_sqrt']].plot(x = 'line_item_value', y = 'line_item_value_sqrt', kind = 'scatter')\n",
        "df_SC[['line_item_value_sqrt']].plot(kind = 'hist')\n",
        "\n",
        "df_SC[['line_item_value','line_item_value_log']].plot(x = 'line_item_value', y = 'line_item_value_log', kind = 'scatter')\n",
        "df_SC[['line_item_value_log']].plot(kind = 'hist')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l07z-nrdVji"
      },
      "source": [
        "### Dummy Variables\n",
        "A categorical variable should generally be encoded as **dummy variables** (a.k.a. indicator variables), each taking only one of two values (0 or 1; False or True)\n",
        "When a categorical variable takes k possible values, you typically have two options to define your dummy variables:\n",
        "* Option 1: Define k-1 dummy variables, and use the unassigned category as the reference category. This is often referred to as **dummy** encoding. This is typically a preferred option to avoid multicolinearity (dummy variable trap: see [link](https://en.wikipedia.org/wiki/Dummy_variable_(statistics))) when using linear regression technique.\n",
        "* Option 2: Define k dummy variables. Often referred to as **one-hot** encoding. This is more commonly used when employing modern machine learning algorithms which include regularization. In this case, the one additional input of all zeroes would represent a missing value (in addition to the categorical values as in dummy encoding).\n",
        "\n",
        "Let's transform our categorical variable `shipment` using these two options. First, let's take a look at the possible values for the categorical values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Scy19RTedVjj"
      },
      "source": [
        "df_SC['shipment_mode']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrxVTOsIdVjj"
      },
      "source": [
        "We will create 4 dummy variables with names `'Air'`,  `'Truck'`, `'Air Charter'` and `'Ocean'`, and use `'missing'` as our reference category. One way to do this is by making use of the `DataFrame` function `pd.get_dummies()`, which automatically  converts categorical variable into dummy/indicator variables. If you  You can check [this page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) for more information. Then, we can add the new `DataFrame` we just created (`df_dummies`) to our original `DataFrame`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5omkP3cjHgUS"
      },
      "source": [
        "df_onehot = pd.get_dummies(df_SC['shipment_mode'])\n",
        "df_onehot.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9eymnJqdVjj"
      },
      "source": [
        "df_dummies = pd.get_dummies(df_SC['shipment_mode'], drop_first=True)\n",
        "df_dummies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3try2FUdVjj"
      },
      "source": [
        "Then, we can add the new `DataFrame` we just created (`df_dummies`) to our original `DataFrame`.  We can do this using the function `pd.concat()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0SC59ivFdVjk"
      },
      "source": [
        "# concatenating the original df_SC with df_dummy\n",
        "df_SC = pd.concat([df_SC, df_onehot], axis=1)\n",
        "df_SC[['shipment_mode']+list(df_onehot.columns)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McAIElwKMXJb"
      },
      "source": [
        "df_SC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sd1yXEVfdVjk"
      },
      "source": [
        "## Transforming numerical variables into categorical variables\n",
        "In some cases, categorical variables may be preferred over numerical ones. We then need to partition the numerical variables into bins according to a specific criteria.\n",
        "As an example, let's transform our original variable `weight_kilograms` into a categorical variable with values `'light'` (if the weight is up to 100 kg), `'medium'`(if the weight is within the interval (100 kg, 500 kg]), `'heavy'` (if the weight is within the interval (500 kg, 1000 kg]) and `'super-heavy'`(if the weight is > 1000 kg).\n",
        "\n",
        "We can implement this transformation using the function `pd.cut()`, which helps us to segment and sort data values into bins. You can check [this page](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html) for more information on this function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgcx3yVSdVjk"
      },
      "source": [
        "bins = [0, 100., 500., 1000.,  float('inf')]             # defining the bins\n",
        "names = ['light', 'medium', 'heavy', 'super-heavy']      # defining the names for the categories\n",
        "df_SC['weight_category'] = pd.cut(df_SC['weight_kilograms'], bins, labels=names, include_lowest=True)  # adding the new cat. var. to our DF\n",
        "df_SC[['weight_kilograms','weight_category']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH_qvb9AOfzk"
      },
      "source": [
        "save_file = \"transformed_df.csv\"\n",
        "df_SC.to_csv(save_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09kxuc0pOgJa"
      },
      "source": [
        "#for Colab, you need to run this one to download the file above\n",
        "from google.colab import files\n",
        "files.download(save_file)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}