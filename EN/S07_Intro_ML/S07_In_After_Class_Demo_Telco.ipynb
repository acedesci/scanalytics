{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "S7-Full_Demo_Telco_tree_visualization.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acedesci/scanalytics/blob/master/EN/S07_Intro_ML/S07_In_After_Class_Demo_Telco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H797invmatYz"
      },
      "source": [
        "# Session 7: Introduction to ML - Classification\n",
        "---\n",
        "*Created by Yossiri Adulyasak*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyTCwtrpatY1"
      },
      "source": [
        "## Demo: Classification on Customer Churn Dataset - full-scale with tree exploration and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke5yJeXbatY2"
      },
      "source": [
        "Classification using customer service churn dataset (https://www.kaggle.com/blastchar/telco-customer-churn).\n",
        "\n",
        "This notebook is used to demonstrate the pipeline runs on the full dataset. You can review this once you are familiar with the main pipeline and building blocks of the predictive task. For the detailed description of each step, please refer to the other simple notebook example of Telco."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvWk3rsPb5-Z"
      },
      "source": [
        "!pip install -q dtreeviz\n",
        "import dtreeviz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC0hkjqkatY3"
      },
      "source": [
        "import sklearn\n",
        "from sklearn import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGKLW3U2DPsd"
      },
      "source": [
        "## **Blocks 1 and 2**: data input and feature preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPGEQHjP4JGL"
      },
      "source": [
        "import pandas as pd\n",
        "# Load transformed dataset\n",
        "url = 'https://raw.githubusercontent.com/acedesci/scanalytics/master/EN/S07_Intro_ML/data/Telco-Customer-Churn_dummies.csv'\n",
        "customer_data = pd.read_csv(url)\n",
        "\n",
        "#take all columns except the first one (ID) until the column prior to the predicted value (the last column is the label y)\n",
        "selected_features = customer_data.columns[1:-1].values\n",
        "\n",
        "print(selected_features)\n",
        "X = customer_data[selected_features].values\n",
        "y = customer_data['Churn_Yes'].values\n",
        "customer_data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KslAXQcRatZD"
      },
      "source": [
        "# Split into training and testing data (72/25 by default)\n",
        "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=0)\n",
        "\n",
        "print(\"N. Training:\", len(y_train), \", N. Testing:\", len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3sfNUCADVOk"
      },
      "source": [
        "## **Blocks 3 and 4**: Model and validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OurT10YPatZH"
      },
      "source": [
        "### Model 1: Logistic regression\n",
        "\n",
        "In addition to precision and recall, we can also measure the aggregate measure of both which is called F-Score (see [link](https://en.wikipedia.org/wiki/F-score)). The F-1 Score is a harmonic mean of both measures where `F-Score = 1.0`  indicates the perfect precision and recall whereas the lowest value is `F-Score = 0` which occurs when either precision or recall equals zero. F-Score score can be calculated as follows:\n",
        "\n",
        "$F_1 = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}} = 2\\frac{Precision \\times Recall}{Precision + Recall}$\n",
        "\n",
        "We can use sklearn function `.f1_score(y_true, y_predict)` to calculate it (see [link](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB2O6pxeatZI"
      },
      "source": [
        "# Run Logistic regression\n",
        "logreg = sklearn.linear_model.LogisticRegression().fit(X_train, y_train)\n",
        "\n",
        "# Print the results\n",
        "print(\"Logistic Regression: Training accuracy: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
        "print(\"Logistic Regression: Testing accuracy: {:.3f}\".format(logreg.score(X_test, y_test)))\n",
        "\n",
        "# Predict the values y from the trained model on the test set\n",
        "y_predict = logreg.predict(X_test)\n",
        "print(\"Logistic Regression: Precision: {:.3f}\".format(sklearn.metrics.precision_score(y_test, y_predict)))\n",
        "print(\"Logistic Regression: Recall: {:.3f}\".format(sklearn.metrics.recall_score(y_test, y_predict)))\n",
        "print(\"Logistic Regression: F1-Score: {:.3f}\".format(sklearn.metrics.f1_score(y_test, y_predict)))\n",
        "\n",
        "# We can also print out a confusion matrix\n",
        "print(\"Display confusion matrix\")\n",
        "print(sklearn.metrics.confusion_matrix(y_test, y_predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNyGfq19atZM"
      },
      "source": [
        "### Model 2: Decision tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zKObEf0o6s0"
      },
      "source": [
        "Here we try the decision tree with different `max_depth` to evaluate the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT4g8l9datZN"
      },
      "source": [
        "# Run decision tree\n",
        "\n",
        "tree_results = pd.DataFrame(columns = ['Depth','Train-Accuracy','Test-Accuracy','Precision','Recall', 'F-Score'])\n",
        "best_depth = 1\n",
        "\n",
        "for depth in range(2,8):\n",
        "  tree = sklearn.tree.DecisionTreeClassifier(max_depth = depth, random_state=0).fit(X_train, y_train)\n",
        "  y_predict = tree.predict(X_test)\n",
        "\n",
        "  # compute each measure and add to the new row in DataFrame\n",
        "  train_acc = tree.score(X_train, y_train) # accuracy on the training set\n",
        "  test_acc = tree.score(X_test, y_test)  # accuracy on the test set\n",
        "  test_precision = sklearn.metrics.precision_score(y_test, y_predict) # precision on the test set\n",
        "  test_recall = sklearn.metrics.recall_score(y_test, y_predict) # recall on the test set\n",
        "  f_score = 2*test_precision*test_recall/(test_precision+test_recall) # f-score on the test set (manual calculation)\n",
        "\n",
        "  # add the results (list) to the last row in the DataFrame\n",
        "  tree_results.loc[len(tree_results)] = [depth,train_acc,test_acc,test_precision,test_recall,f_score]\n",
        "\n",
        "  # we keep track of the best depth\n",
        "  if f_score >= max(tree_results['F-Score']):\n",
        "    best_depth = depth\n",
        "\n",
        "print(\"Best depth = \", best_depth)\n",
        "tree_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ4GzB70E7Yg"
      },
      "source": [
        "We can also plot the feature importance to explore the insights of the best tree (based on F-score)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnvUz-pbE7no"
      },
      "source": [
        "# retrain the model using the best depth\n",
        "tree = sklearn.tree.DecisionTreeClassifier(max_depth = best_depth, random_state=0).fit(X_train, y_train)\n",
        "\n",
        "# obtain feature importance scores and save in dataframe\n",
        "feature_df = pd.DataFrame(index = selected_features)\n",
        "feature_df['Importance'] = tree.feature_importances_\n",
        "feature_df =feature_df[feature_df['Importance'] > 0].sort_values('Importance')\n",
        "display(feature_df)\n",
        "\n",
        "feature_df.plot(kind='barh', figsize = (5,7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp6gjP0AU1Gw"
      },
      "source": [
        "If you want to obtain the probability associated with the positive class (label = 1) rather than the binary value. We can use the function `.predict_proba(X)` [link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba). This function returns a 2-D array of pairs of probabilities associated with label 0 and label 1 so we can slice only the second element from each data point by using `[:, 1]` (see [link](https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/) on slicing of 2-D array)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lC4coWrU1P5"
      },
      "source": [
        "# y_prob = logreg.predict_proba(X_test)[:, 1] # use this for logistic regression\n",
        "y_prob = tree.predict_proba(X_test)[:, 1] # use this for tree\n",
        "print(y_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9ue6p9cDhCD"
      },
      "source": [
        "### Tree exploration and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISMYZ-LGeyN_"
      },
      "source": [
        "In the lecture, we plot the tree in `sklearn` using the function [link text] (https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html). However, we might consider using a better visualization from `dtreeviz` package [link1](https://github.com/parrt/dtreeviz), [link2](https://github.com/parrt/dtreeviz/blob/master/notebooks/dtreeviz_sklearn_visualisations.ipynb) and [link3](https://explained.ai/decision-tree-viz/) to visualize the tree. This package can be slow to run in particular with a large dataset and deep tree so you should limit the tree size when using it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYKoNxzPcTWj"
      },
      "source": [
        "tree = sklearn.tree.DecisionTreeClassifier(max_depth = 3, random_state=0).fit(X_train, y_train)\n",
        "viz = dtreeviz.model(tree, X_train, y_train, target_name=\"Churn\", feature_names=selected_features)\n",
        "viz.view()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlDqeFq8Bi51"
      },
      "source": [
        "You can also save the tree visualization by calling the function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha6hVQ1s_zIq"
      },
      "source": [
        "viz.view().save('tree.svg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk4Fpsn4BoZ2"
      },
      "source": [
        "For Colab, we also need to download it again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfbItopKBb_v"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('tree.svg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx7U9twUBw-W"
      },
      "source": [
        "Another use of this library is to display a specific path of the prediction for a single data point. Here we highlight a specific path on the tree."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "viz.view(x=X_test[4])"
      ],
      "metadata": {
        "id": "pLb_aCmjFDMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcB1wSMrCp16"
      },
      "source": [
        "Alternatively, you can also show a single path (rather than the entire tree)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d3kLv4kCg_6"
      },
      "source": [
        "viz.view(x=X_test[4], show_just_path=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}