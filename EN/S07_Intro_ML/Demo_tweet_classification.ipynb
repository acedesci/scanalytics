{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acedesci/scanalytics/blob/master/EN/S07_Intro_ML/Demo_tweet_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to ML - Text Classification**\n",
        "\n",
        "![Airline_tweets](https://raw.githubusercontent.com/acedesci/scanalytics/refs/heads/master/EN/S07_Intro_ML/data/illustration_v2.jpg)"
      ],
      "metadata": {
        "id": "PKxk-Q1J05YH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# IMPORTS & SETUP\n",
        "# ==========================================\n",
        "# Import pandas for data manipulation and analysis\n",
        "import pandas as pd\n",
        "# Import numpy for numerical operations, especially with arrays\n",
        "import numpy as np\n",
        "# Import matplotlib.pyplot for basic plotting functionalities\n",
        "import matplotlib.pyplot as plt\n",
        "# Import seaborn for advanced data visualization\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-Learn (Traditional ML) - A library for machine learning in Python\n",
        "from sklearn.model_selection import train_test_split # Used for splitting data into training and testing sets\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Used for converting text data into numerical TF-IDF features\n",
        "from sklearn.tree import DecisionTreeClassifier # Used for building a Decision Tree classification model\n",
        "from sklearn.linear_model import LogisticRegression  # Baseline Linear Model for classification\n",
        "from sklearn.metrics import classification_report # Used for evaluating classification model performance\n",
        "from sklearn.preprocessing import LabelEncoder # Used for encoding categorical labels into numerical format\n",
        "\n",
        "# LightGBM (Gradient Boosting) - A high-performance gradient boosting framework\n",
        "import lightgbm as lgb # Used for building a LightGBM classification model\n",
        "\n",
        "# Deep Learning (Keras/TensorFlow) - Libraries for building and training neural networks\n",
        "import tensorflow as tf # Core TensorFlow library\n",
        "from tensorflow.keras.models import Sequential # Used for building sequential Keras models\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, GlobalAveragePooling1D # Various Keras layers for neural networks\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # Used for converting text into sequences of integers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # Used for padding sequences to a uniform length"
      ],
      "metadata": {
        "id": "sxegHB4HPMr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Block 1:** data input"
      ],
      "metadata": {
        "id": "qIbfx85KfHrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 1. LOAD DATA (Airline Tweets)\n",
        "# ==========================================\n",
        "print(\"Downloading dataset...\")\n",
        "# URL of the dataset to be downloaded\n",
        "url = \"https://raw.githubusercontent.com/acedesci/scanalytics/refs/heads/master/EN/S07_Intro_ML/data/tweets_data_filtered.csv\"\n",
        "# Read the CSV file from the URL into a pandas DataFrame\n",
        "df = pd.read_csv(url)\n",
        "# Display the entire DataFrame (or its head if it's very large)\n",
        "df"
      ],
      "metadata": {
        "id": "MQNAhZQmPMxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Block 2:** Feature preparation & Feature engineering\n",
        "\n",
        "## Term Frequency - Inverse Document Frequency (TF-IDF) features for text data\n",
        "\n",
        "We use the following line of code to process the text:\n",
        "`TfidfVectorizer(stop_words='english', max_features=1000)`\n",
        "\n",
        "Here is what it does:\n",
        "* **`TfidfVectorizer`:** This tool converts words into numbers (values). Instead of just counting words, it assigns a \"Uniqueness Score.\"\n",
        "    * **Common words** get low scores.\n",
        "    * **Rare words** get high scores.\n",
        "* **`stop_words='english'`:** This tells the computer to ignore common grammatical fillers that carry no meaning, such as *\"the\"*, *\"is\"*, *\"and\"*, *\"of\"*.\n",
        "* **`max_features=1000`:** This forces the model to focus only on the **top 1,000 most important words** in the entire dataset. It ignores rare typos or irrelevant words.\n",
        "\n",
        "---\n",
        "\n",
        "## How is the TF-IDF score Calculated?\n",
        "The TF-IDF score is the product of two numbers: **Term Frequency (TF)** and **Inverse Document Frequency (IDF)**.\n",
        "\n",
        "$$\\text{TF-IDF} = \\text{TF} \\times \\text{IDF}$$\n",
        "\n",
        "### Step 1: Term Frequency (TF)\n",
        "*\"How often does the word appear in THIS specific text?\"*\n",
        "If a customer writes \"late\" 5 times, it is very important to that specific email.\n",
        "\n",
        "$$\\text{TF} = \\frac{\\text{Count of word in document}}{\\text{Total words in document}}$$\n",
        "\n",
        "### Step 2: Inverse Document Frequency (IDF)\n",
        "*\"How rare is this word across ALL data points/rows (tweets)?\"* This is the penalty for generic words.\n",
        "* If a word appears in **every** tweet, the IDF is **low** (near 0).\n",
        "* If a word appears in **only one** tweet (e.g., \"shattered\"), the IDF is **high**.\n",
        "\n",
        "$$\\text{IDF} = \\ln \\left( \\frac{1+\\text{Total Number of Documents}}{1+\\text{Number of Documents containing the word}} \\right)+1$$\n",
        "\n",
        "### A Simple Example\n",
        "Based on the following 3 short tweets:\n",
        "1.  **Doc A:** \"Delivery late\"\n",
        "2.  **Doc B:** \"Delivery fast\"\n",
        "3.  **Doc C:** \"Package lost\"\n",
        "\n",
        "**Why does \"Late\" get a higher score than \"Delivery\"?**\n",
        "\n",
        "| Word | Found In | Logic | Score |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **\"Delivery\"** | Doc A & Doc B | It is **more common** (found in 2/3 docs). The model thinks it is less important. | **0.644** |\n",
        "| **\"Late\"** | Doc A only | It is **more unique** (found in 1/3 docs). The model highlights it as a key signal. | **0.847** |\n",
        "\n",
        "**Note:**\n",
        "*  \"Delivery (Tweet A)\": $\\text{TF} = 0.5, \\text{IDF} = \\ln\\left( \\frac{1+3}{1+2} \\right)+1=1.12, \\text{TF-IDF} = 0.5\\times1.288 = 0.644$\n",
        "*  \"Late (Tweet A)\": $\\text{TF} = 0.5, \\text{IDF} = \\ln\\left( \\frac{1+3}{1+1} \\right)+1=1.30, \\text{TF-IDF} = 0.5\\times1.693 = 0.847$\n",
        "\n",
        "\n",
        "**Result:** The model ignores the generic topic (\"Delivery\") and focuses purely on the problem (\"Late\")."
      ],
      "metadata": {
        "id": "fpg7QZxuY9Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files # Import files module for Google Colab to download files\n",
        "\n",
        "# ==========================================\n",
        "# PREPROCESSING & SPLIT\n",
        "# ==========================================\n",
        "# Initialize LabelEncoder to convert categorical labels to numerical ones\n",
        "le = LabelEncoder()\n",
        "# Apply LabelEncoder to the 'category' column to create numerical labels\n",
        "df['label'] = le.fit_transform(df['category'])\n",
        "# Determine the number of unique classes after encoding\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# X_train_text, X_test_text: text data for training and testing\n",
        "# y_train, y_test: corresponding numerical labels for training and testing\n",
        "# test_size=0.2: 20% of data for testing, 80% for training\n",
        "# random_state=42: Ensures reproducibility of the split\n",
        "# stratify=df['label']: Ensures that the proportion of classes is the same in train and test sets\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
        "    df['text'], df['label'], test_size=0.2, random_state=42, stratify=df['label']\n",
        ")\n",
        "\n",
        "# Print the number of samples in the training and testing sets\n",
        "print(f\"\\nTraining on {len(X_train_text)} samples | Testing on {len(X_test_text)} samples\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. FEATURE ENGINEERING (TF-IDF)\n",
        "# ==========================================\n",
        "# Shared by Tree, Logistic Regression, and LightGBM models\n",
        "# Initialize TfidfVectorizer:\n",
        "# stop_words='english': Removes common English stop words (e.g., 'the', 'is', 'a')\n",
        "# max_features=1000: Considers only the top 1000 most frequent words/terms\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "\n",
        "# Fit the TF-IDF vectorizer on the training text data and transform it into a TF-IDF matrix\n",
        "# .toarray() converts the sparse matrix output to a dense NumPy array\n",
        "X_train_tfidf = tfidf.fit_transform(X_train_text).toarray()\n",
        "\n",
        "# Transform the testing text data using the *fitted* TF-IDF vectorizer\n",
        "X_test_tfidf = tfidf.transform(X_test_text).toarray()"
      ],
      "metadata": {
        "id": "EabypFHoQCeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: this part of code is optional. It is to show and export the TD-IDF input features\n",
        "# Create DataFrame for first 100 rows of the TF-IDF training data to display\n",
        "# Get the feature names (words) from the TF-IDF vectorizer\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "# Create a pandas DataFrame from the first 50 rows of X_train_tfidf, using feature_names as columns\n",
        "tfidf_df = pd.DataFrame(X_train_tfidf[:50], columns=feature_names)\n",
        "# Define the filename for saving the TF-IDF sample as a CSV\n",
        "csv_filename = 'tfidf_sample.csv'\n",
        "# Save the sample TF-IDF DataFrame to a CSV file without the index\n",
        "tfidf_df.to_csv(csv_filename, index=False)\n",
        "# Download the generated CSV file to the local machine (specific to Google Colab)\n",
        "files.download(csv_filename)\n",
        "# Display the head of the TF-IDF DataFrame\n",
        "tfidf_df"
      ],
      "metadata": {
        "id": "PCjuIL_af6Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Blocks 3 & 4:** Machine Learning Models - Training and Validation"
      ],
      "metadata": {
        "id": "AjWziPmwgFuP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model A: Decision Tree"
      ],
      "metadata": {
        "id": "99IDhpNqiBhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model A: Decision Tree ---\n",
        "print(\"\\n--- Training Model A: Decision Tree ---\")\n",
        "# Initialize Decision Tree Classifier with a maximum depth of 10 and a fixed random state for reproducibility\n",
        "tree_model = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
        "# Train the Decision Tree model using the TF-IDF transformed training data and labels\n",
        "tree_model.fit(X_train_tfidf, y_train)\n",
        "# Make predictions on the TF-IDF transformed test data\n",
        "tree_preds = tree_model.predict(X_test_tfidf)\n",
        "# Print the classification report, showing precision, recall, f1-score, and support for each class\n",
        "print(classification_report(y_test, tree_preds, target_names=le.classes_))"
      ],
      "metadata": {
        "id": "JGn3qRqqiAQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model B: Logistic Regression"
      ],
      "metadata": {
        "id": "bI9bs18QiH4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model B: Logistic Regression ---\n",
        "print(\"\\n--- Training Model B: Logistic Regression ---\")\n",
        "# Excellent baseline for text. Linear boundary is often enough for \"keywords\".\n",
        "# Initialize Logistic Regression model with a maximum of 1000 iterations and a fixed random state\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "# Train the Logistic Regression model using the TF-IDF transformed training data and labels\n",
        "lr_model.fit(X_train_tfidf, y_train)\n",
        "# Make predictions on the TF-IDF transformed test data\n",
        "lr_preds = lr_model.predict(X_test_tfidf)\n",
        "# Print the classification report\n",
        "print(classification_report(y_test, lr_preds, target_names=le.classes_))"
      ],
      "metadata": {
        "id": "pJvtYveYiMvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model C: LightGBM"
      ],
      "metadata": {
        "id": "vtYJZWOTiOK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model C: LightGBM ---\n",
        "print(\"\\n--- Training Model C: LightGBM ---\")\n",
        "# Gradient boosting: Often the strongest performer on tabular/sparse data.\n",
        "# We use the Scikit-Learn API wrapper of LightGBM for simplicity\n",
        "# Initialize LightGBM Classifier with a fixed random state and suppress verbose output\n",
        "lgb_model = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
        "# Train the LightGBM model using the TF-IDF transformed training data and labels\n",
        "lgb_model.fit(X_train_tfidf, y_train)\n",
        "# Make predictions on the TF-IDF transformed test data\n",
        "lgb_preds = lgb_model.predict(X_test_tfidf)\n",
        "# Print the classification report\n",
        "print(classification_report(y_test, lgb_preds, target_names=le.classes_))\n",
        "\n"
      ],
      "metadata": {
        "id": "v0kbb6ejOYOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Supplement) Model D: Deep Learning with word embedding layer\n",
        "(NOTE: the detail of the model is not covered in this class as it requires deeper understanding of the structure of the deep learning model layers)"
      ],
      "metadata": {
        "id": "pNNg5_ttiTbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NOTE: What is an \"Embedding\" Layer?\n",
        "\n",
        "In simple terms, the **Embedding Layer** acts as a **Smart Translator**. When we tokenize text, we turn words into integers:\n",
        "* \"Late\" $\\rightarrow$ 45\n",
        "* \"Delayed\" $\\rightarrow$ 982\n",
        "* \"Happy\" $\\rightarrow$ 12\n",
        "\n",
        "To a computer, **45** and **982** look completely unrelated. The math assumes \"Delayed\" (982) is much \"bigger\" than \"Late\" (45), which is false as they mean the same thing.\n",
        "\n",
        "\n",
        "The Embedding layer gives every word a specific location in a multi-dimensional map (a vector of numbers). Instead of just `45`, the word **\"Late\"** becomes a list of coordinates, like: `[0.2, -0.5, 0.9, ...]`\n",
        "\n",
        "**Embedding later in deep learning:**\n",
        "During training, the model moves words with similar meanings closer together on this map.\n",
        "* **\"Late\"** and **\"Delayed\"** will end up with very similar coordinates.\n",
        "* **\"Late\"** and **\"Happy\"** will end up far apart.\n",
        "\n",
        "For example, after training, when plotting words on a 2D map (two coordinates):\n",
        "\n",
        "| Word | Dimension A (Positivity) | Dimension B (Urgency) |\n",
        "| :--- | :--- | :--- |\n",
        "| **\"Late\"** | -0.9 (Bad) | 0.9 (High) |\n",
        "| **\"Delayed\"** | -0.8 (Bad) | 0.9 (High) |\n",
        "| **\"Good\"** | 0.9 (Good) | 0.1 (Low) |\n",
        "\n",
        "You can see how \"Late\" and \"Delayed\" have almost the same numbers. This helps the Deep Learning model understand that they are synonyms. This transformation is directly integrated into the deep learning model during training (i.e., feature engineering is **automatically** performed rather than pre-calculated like TD-IDF)."
      ],
      "metadata": {
        "id": "Ual0ed3ulBpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model D: Deep Learning (Embeddings) ---\n",
        "print(\"\\n--- Training Model D: Deep Learning ---\")\n",
        "# Preprocessing unique to DL\n",
        "# Define vocabulary size for the Tokenizer\n",
        "vocab_size = 2000\n",
        "# Define the dimension of the embedding vectors\n",
        "embedding_dim = 16\n",
        "# Define the maximum length of input sequences\n",
        "max_length = 30\n",
        "# Define truncation type ('post' means truncate from the end)\n",
        "trunc_type = 'post'\n",
        "\n",
        "# Initialize Tokenizer with the specified vocabulary size and an out-of-vocabulary token\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "# Fit the tokenizer on the training text data to build the word index\n",
        "tokenizer.fit_on_texts(X_train_text)\n",
        "\n",
        "# Convert training text into sequences of integers\n",
        "train_sequences = tokenizer.texts_to_sequences(X_train_text)\n",
        "# Convert testing text into sequences of integers\n",
        "test_sequences = tokenizer.texts_to_sequences(X_test_text)\n",
        "\n",
        "# Pad training sequences to a uniform length (max_length)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, truncating=trunc_type)\n",
        "# Pad testing sequences to a uniform length (max_length)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "# Build the Deep Learning model (Sequential Keras model)\n",
        "dl_model = Sequential([\n",
        "    # Embedding layer: Converts integer-encoded words into dense vectors of fixed size\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    # GlobalAveragePooling1D layer: Averages the embedding vectors across the sequence dimension\n",
        "    GlobalAveragePooling1D(),\n",
        "    # Dense layer with 24 units and ReLU activation function\n",
        "    Dense(24, activation='relu'),\n",
        "    # Dropout layer: Randomly sets a fraction of input units to 0 at each update during training to prevent overfitting\n",
        "    Dropout(0.5),\n",
        "    # Output Dense layer with 'num_classes' units and 'softmax' activation for multi-class classification probabilities\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model:\n",
        "# loss='sparse_categorical_crossentropy': Suitable for integer-encoded labels\n",
        "# optimizer='adam': An adaptive learning rate optimization algorithm\n",
        "# metrics=['accuracy']: Metric to monitor during training and evaluation\n",
        "dl_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# Train the Deep Learning model:\n",
        "# epochs=20: Number of times to iterate over the entire training dataset\n",
        "# validation_data=(test_padded, y_test): Data on which to evaluate the loss and any model metrics at the end of each epoch\n",
        "# verbose=0: Suppress training output\n",
        "dl_model.fit(train_padded, y_train, epochs=20, validation_data=(test_padded, y_test), verbose=0)\n",
        "\n",
        "# Evaluate the trained Deep Learning model on the test data\n",
        "loss, accuracy = dl_model.evaluate(test_padded, y_test, verbose=0)\n",
        "# Print the accuracy of the Deep Learning model\n",
        "print(f\"Deep Learning Accuracy: {accuracy*100:.2f}%\")\n",
        "# Make predictions with the Deep Learning model on the test data\n",
        "# np.argmax converts the probability distribution output to class labels\n",
        "dl_preds = np.argmax(dl_model.predict(test_padded, verbose=0), axis=1)\n",
        "# Print the classification report for the Deep Learning model\n",
        "print(classification_report(y_test, dl_preds, target_names=le.classes_))"
      ],
      "metadata": {
        "id": "2wfRaj5rh-In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code below if you want to display the illustration of the neural network model above"
      ],
      "metadata": {
        "id": "mw6il34UjNFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# The dl_model was already built and trained in the previous cell,\n",
        "# so explicitly calling .build() here is not necessary. We can remove this line.\n",
        "# dl_model.build(input_shape=(None, max_length))\n",
        "\n",
        "# ============================================\n",
        "# GENERATE DIAGRAM\n",
        "# ============================================\n",
        "print(\"Generating Model Architecture Diagram...\")\n",
        "\n",
        "plot_path = 'model_architecture.png'\n",
        "\n",
        "# Plot the Deep Learning model (dl_model)\n",
        "plot_model(dl_model,\n",
        "           to_file=plot_path,\n",
        "           show_shapes=True,      # Shows inputs/outputs (e.g., 30 words -> 16 numbers)\n",
        "           show_layer_names=True, # Shows the custom names we defined above\n",
        "           rankdir='TB',          # TB = Top to Bottom flow\n",
        "           dpi=100                # Image resolution\n",
        ")\n",
        "\n",
        "# Display the image inline\n",
        "display(Image(plot_path))"
      ],
      "metadata": {
        "id": "6Q0mXjDLjH2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prediction:** show live prediction based on the text input\n",
        "(this widget is vibe-coded created using Gemini üòè)"
      ],
      "metadata": {
        "id": "WYk4iaqYgYN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive Python widget\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# ==========================================\n",
        "# INTERACTIVE WIDGET\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"INTERACTIVE CLASSIFIER (With Confidence Scores)\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "text_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Type simulated review...',\n",
        "    description='Log:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "out = widgets.Output()\n",
        "\n",
        "def get_prediction_and_confidence(model, vector):\n",
        "    # predict_proba returns array like [[0.1, 0.8, 0.1]]\n",
        "    probs = model.predict_proba(vector)[0]\n",
        "    idx = np.argmax(probs)\n",
        "    label = le.classes_[idx]\n",
        "    confidence = np.max(probs) * 100\n",
        "    return label, confidence\n",
        "\n",
        "def on_text_submit(change):\n",
        "    with out:\n",
        "        clear_output()\n",
        "        input_text = change.new\n",
        "        if not input_text: return\n",
        "\n",
        "        print(f\"Analysis for: '{input_text}'\\n\" + \"-\"*30)\n",
        "\n",
        "        # Preprocessing\n",
        "        vec = tfidf.transform([input_text]).toarray()\n",
        "\n",
        "        # 1. Tree\n",
        "        # Trees can output probabilities (fraction of samples in the leaf)\n",
        "        t_lbl, t_conf = get_prediction_and_confidence(tree_model, vec)\n",
        "        print(f\"üå≤ Decision Tree:       [{t_lbl}] (Conf: {t_conf:.1f}%)\")\n",
        "\n",
        "        # 2. Logistic Regression\n",
        "        lr_lbl, lr_conf = get_prediction_and_confidence(lr_model, vec)\n",
        "        print(f\"üìà Logistic Regression: [{lr_lbl}] (Conf: {lr_conf:.1f}%)\")\n",
        "\n",
        "        # 3. LightGBM\n",
        "        gb_lbl, gb_conf = get_prediction_and_confidence(lgb_model, vec)\n",
        "        print(f\"üöÄ LightGBM:            [{gb_lbl}] (Conf: {gb_conf:.1f}%)\")\n",
        "\n",
        "        # 4. Deep Learning\n",
        "        seq = tokenizer.texts_to_sequences([input_text])\n",
        "        padded = pad_sequences(seq, maxlen=max_length)\n",
        "        dl_probs = dl_model.predict(padded, verbose=0)[0]\n",
        "        dl_idx = np.argmax(dl_probs)\n",
        "        dl_conf = np.max(dl_probs) * 100\n",
        "        print(f\"üß† Deep Learning:       [{le.classes_[dl_idx]}] (Conf: {dl_conf:.1f}%)\")\n",
        "\n",
        "text_input.observe(on_text_submit, names='value')\n",
        "display(text_input)\n",
        "display(out)"
      ],
      "metadata": {
        "id": "8w94BOCVg9G1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}